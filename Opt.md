quantize
https://github.com/eladhoffer/quantized.pytorch
ä½¿ç”¨pytorch

æˆ‘ä»¬çœŸçš„éœ€è¦æ¨¡å‹å‹ç¼©å—ï¼Ÿ
http://mitchgordon.me/machine/learning/2020/01/13/do-we-really-need-model-compression.html

### [ECCV 2018] PyTorch implementation for AMC: AutoML for Model Compression and Acceleration on Mobile Devices.
https://github.com/mit-han-lab/amc

ç¥ç»ç½‘ç»œé‡åŒ–ç›¸å…³æ–‡çŒ®é›†
https://github.com/xu3kev/neural-networks-quantization-notes

Pytorch Implementation of Neural Architecture Optimization
https://github.com/renqianluo/NAO_pytorch

XNNPACKï¼šé¢å‘æ‰‹æœºå’Œæµè§ˆå™¨çš„é«˜æ•ˆæµ®ç‚¹ç¥ç»ç½‘ç»œæ¨ç†è¿ç®—å™¨
https://github.com/google/XNNPACK

TensorRT æ·±åº¦å­¦ä¹ ä¼˜åŒ–
https://github.com/ardianumam/Tensorflow-TensorRT

prune
https://github.com/jacobgil/pytorch-pruning
https://jacobgil.github.io/deeplearning/pruning-deep-learning
2016å¹´çš„ç®—æ³•

https://github.com/Eric-mingjie/rethinking-network-pruning
æä¾›äº†6ç§ç½‘ç»œå‰ªææ–¹æ³•ï¼Œä½¿ç”¨pytorch
https://github.com/alexfjw/prunnable-layers-pytorch

ã€ç”¨TensorRTç½‘ç»œå®šä¹‰APIå®ç°æµè¡Œçš„æ·±åº¦å­¦ä¹ ç½‘ç»œã€‘
https://github.com/wang-xinyu/tensorrtx

ã€é‡åŒ–å™ªå£°è®­ç»ƒæé™æ¨¡å‹å‹ç¼©ã€‘
https://github.com/pytorch/fairseq/tree/master/examples/quant_noise

### (ç»¼è¿°)é¢å‘ç§»åŠ¨è®¾å¤‡/è¾¹ç¼˜è®¡ç®—çš„æ–°å‹ç¥ç»ç½‘ç»œæ¶æ„
https://machinethink.net/blog/mobile-architectures/

Pruning AI networks without impacting performance
https://github.com/DNNToolBox/Net-Trim-v1

åŸºäºKerasçš„AutoMLæœºå™¨å­¦ä¹ è‡ªåŠ¨åŒ–åº“
https://github.com/jhfjhfj1/autokeras

prune & quantize
https://github.com/NervanaSystems/distiller
intelå¼€æºçš„ç½‘ç»œè’¸é¦å·¥å…·ï¼Œä½¿ç”¨pytorchæ¡†æ¶ï¼Œä¸æ–­æ›´æ–°ä¸­ã€‚ã€‚ã€‚
æ•™ç¨‹ï¼šhttps://github.com/NervanaSystems/distiller/wiki/Tutorial:-Using-Distiller-to-prune-a-PyTorch-language-model

tencent
https://github.com/Tencent/PocketFlow

è‡ªåŠ¨åŒ–è¶…å‚æ•°æœç´¢ï¼Œåé¢å¯æ¥å¾ˆå¤šä¸åŒæ¡†æ¶
https://github.com/tobegit3hub/advisor

ç½‘ç»œç»“æ„æœç´¢ï¼Œè°·æ­Œå‡ºå“
https://github.com/tensorflow/adanet

Slimmable Neural Networks
https://github.com/JiahuiYu/slimmable_networks
https://github.com/JiahuiYu/slimmable_networks/tree/detection

Trained Rank Pruning for Efficient Deep Neural Networks
https://github.com/yuhuixu1993/Trained-Rank-Pruning

AutoMLç›¸å…³èµ„æºåˆ—è¡¨
https://github.com/dragen1860/awesome-AutoML

æ·±åº¦ç½‘ç»œæ¨¡å‹å‹ç¼©ä¸åŠ é€Ÿç›¸å…³æ–‡çŒ®å¤§åˆ—è¡¨
https://github.com/sun254/awesome-model-compression-and-acceleration

è¶…å‚ä¼˜åŒ–æ¡†æ¶Optuna
https://github.com/pfnet/optuna

å¾®è½¯å‘å¸ƒçš„AutoMLå·¥å…·åŒ…(è‡ªåŠ¨ç½‘ç»œç»“æ„æœç´¢/è¶…å‚ä¼˜åŒ–)
https://github.com/Microsoft/nni
https://github.com/Microsoft/nni/blob/master/docs/GetStarted.md

Kerasæ¨¡å‹è¶…å‚è°ƒä¼˜å·¥å…·
https://github.com/autonomio/talos

ç¥ç»ç½‘ç»œæœºå™¨å­¦ä¹ è‡ªåŠ¨åŒ–æ¡†æ¶
https://github.com/CiscoAI/amla

PyTorch å®ç°çš„NEAT (NeuroEvolution of Augmenting Topologies)ç¥ç»è¿›åŒ–ç®—æ³•
https://github.com/uber-research/PyTorch-NEAT/

æ¨¡å‹è¶…å‚è‡ªåŠ¨æœç´¢å·¥å…·
https://github.com/NVIDIA/Milano

ç¥ç»ç½‘ç»œæ¨¡å‹é‡åŒ–æ–¹æ³•ç®€ä»‹
http://chenrudan.github.io/blog/2018/10/02/networkquantization.html

Rethinking the Value of Network Pruning
https://github.com/Eric-mingjie/rethinking-network-pruning

åœ¨çº¿è¶…å‚æ•°ä¼˜åŒ–å¹³å°Bender
https://github.com/Dreem-Organization/benderopt


Tensorflow Implementation of ChannelNets (NIPS18) https://arxiv.org/abs/1809.01330
https://github.com/HongyangGao/ChannelNets

Neural Architecture Optimization
https://github.com/renqianluo/NAO

AdaNet ç®€ä»‹ï¼šå¿«é€Ÿçµæ´»çš„ AutoMLï¼Œæä¾›å­¦ä¹ ä¿è¯
https://github.com/tensorflow/adanet
https://mp.weixin.qq.com/s?__biz=MzU1OTMyNDcxMQ==&mid=2247485095&idx=1&sn=990ca481921261e9ff5d850bd3753d6e&chksm=fc184defcb6fc4f986862449c630c80e9f285647a6bcac4a9e03c1108d11554a427d62da148e&scene=0&xtrack=1#rd

Sonnetæ˜¯ä¸ªåŸºäºTensorFlowçš„åº“ï¼Œå¯ä»¥å¸®åŠ©ä½ å»ºç«‹å¤æ‚çš„ç¥ç»ç½‘ç»œã€‚è¯¥é¡¹ç›®ç”±Deepmindçš„Malcolm Reynoldsåˆ›å»ºã€‚
https://github.com/deepmind/sonnet

ç¥ç»ç½‘ç»œç®€åŒ–åº”ç”¨æ¡†æ¶
https://github.com/mindsdb/mindsdb

è…¾è®¯å‘å¸ƒçš„æ¨¡å‹å‹ç¼©è‡ªåŠ¨åŒ–(AutoMC)æ¡†æ¶
https://github.com/Tencent/PocketFlow

é¢å‘æ‰‹æœºä¼˜åŒ–çš„é‡åŒ–ç¥ç»ç½‘ç»œç®—å­åº“
https://github.com/pytorch/QNNPACK

NVIDIA TensorRTï¼šNVIDIA GPUå’Œæ·±åº¦å­¦ä¹ åŠ é€Ÿå™¨çš„C++é«˜æ€§èƒ½æ¨ç†åº“
https://github.com/NVIDIA/TensorRT

PeleeNet: An efficient DenseNet architecture for mobile devices
https://github.com/Robert-JunWang/PeleeNet

æ·±åº¦ç½‘ç»œå‹ç¼©/åŠ é€Ÿæœ€æ–°è¿›å±•åˆ—è¡¨
https://github.com/MingSun-Tse/EfficientDNNs

ç”¨äºå¯»æ‰¾å’Œåˆ†æç¥ç»ç½‘ç»œé‡è¦ç¥ç»å…ƒçš„å·¥å…·åŒ…
https://github.com/fdalvi/NeuroX

å¼€æº:Keras + Hyperoptæ–¹ä¾¿è¶…å‚ä¼˜åŒ–çš„ç®€å•å°è£…Hyperas
http://maxpumperla.com/hyperas/

PyTorchæœºå™¨å­¦ä¹ è‡ªåŠ¨åŒ–ï¼šè‡ªåŠ¨æ¡†æ¶æœç´¢ã€è¶…å‚ä¼˜åŒ–
https://github.com/automl/Auto-PyTorch

Tensorized Embedding Layers for Efficient Model Compression
https://github.com/KhrulkovV/tt-pytorch

### åŸºäºpytorchå®ç°æ¨¡å‹å‹ç¼©
https://github.com/666DZY666/model-compression

DeepSpeedï¼šå¾®è½¯çš„æ·±åº¦å­¦ä¹ ä¼˜åŒ–åº“ï¼Œè®©åˆ†å¸ƒå¼è®­ç»ƒæ›´ç®€å•ã€æ›´é«˜æ•ˆã€æ›´æœ‰æ•ˆ
https://github.com/microsoft/DeepSpeed
https://github.com/microsoft/DeepSpeedExamples

Adlikï¼šæ·±åº¦å­¦ä¹ æ¨¡å‹ç«¯åˆ°ç«¯ä¼˜åŒ–æ¡†æ¶ï¼Œå…¶æ¨¡å‹ç¼–è¯‘å™¨æ”¯æŒå‰ªæã€é‡åŒ–å’Œç»“æ„å‹ç¼©ç­‰å¤šç§ä¼˜åŒ–æŠ€æœ¯ï¼Œå¯ä»¥æ–¹ä¾¿åœ°ç”¨äºä½¿ç”¨ TensorFlowã€ Kerasã€ PyTorch ç­‰å¼€å‘çš„æ¨¡å‹ï¼ŒæœåŠ¡å¹³å°æä¾›åŸºäºéƒ¨ç½²ç¯å¢ƒçš„å…·æœ‰ä¼˜åŒ–è¿è¡Œæ—¶çš„æ·±åº¦å­¦ä¹ æ¨¡å‹
https://github.com/Adlik/Adlik

Code for paper "Learning to Reweight Examples for Robust Deep Learning"
https://github.com/uber-research/learning-to-reweight-examples

Code for â€œDiscrimination-aware-Channel-Pruning-for-Deep-Neural-Networksâ€
https://github.com/SCUT-AILab/DCP

ZeroQ: A Novel Zero Shot Quantization Framework
https://github.com/amirgholami/ZeroQ

Graph Transforms to Quantize and Retrain Deep Neural Nets in TensorFlow. https://arxiv.org/abs/1903.08066
https://github.com/Xilinx/graffitist

A PyTorch implementation of "Incremental Network Quantization: Towards Lossless CNNs with Low-Precision Weights"
https://github.com/Mxbonn/INQ-pytorch

This repository contains the training code of Quantization Networks introduced in our CVPR 2019 paper: Quantization Networks.
https://github.com/aliyun/alibabacloud-quantization-networks

Code released for "FNNP: Fast Neural Network Pruning Using Adaptive Batch Normalization"
https://github.com/anonymous47823493/FNNP

Code for the NuerIPS'19 paper "Gate Decorator: Global Filter Pruning Method for Accelerating Deep Convolutional Neural Networks"
https://github.com/youzhonghui/gate-decorator-pruning

ã€PyTorchå®ç°çš„æ·±åº¦æ¨¡å‹å‹ç¼©ã€‘
https://github.com/666DZY666/model-compression

ã€æ·±åº¦ç½‘ç»œå‹ç¼©æ–‡çŒ®/ä»£ç åˆ—è¡¨ã€‘
https://github.com/csyhhu/Awesome-Deep-Neural-Network-Compression

ã€æ¨¡å‹å‹ç¼©ç›¸å…³æ–‡çŒ®èµ„æºå¤§åˆ—è¡¨ã€‘
https://github.com/ChanChiChoi/awesome-model-compression

ã€ç¥ç»ç½‘ç»œå‹ç¼©ä¸åŠ é€Ÿèµ„æºé›†é”¦ã€‘
https://github.com/mrgloom/Network-Speed-and-Compression

Neural Rejuvenation: Improving Deep Network Training by Enhancing Computational Resource Utilization at CVPR'19
https://github.com/joe-siyuan-qiao/NeuralRejuvenation-CVPR19

### é¢å‘ç›®æ ‡æ£€æµ‹/è¯­ä¹‰åˆ†å‰²çš„æœºå™¨å­¦ä¹ è‡ªåŠ¨åŒ–(AutoML)
https://github.com/NoamRosenberg/AutoML

ã€ç¥ç»ç½‘ç»œä¿®å‰ªæŠ€æœ¯ç ”ç©¶æŒ‡å—ã€‘
https://pan.baidu.com/s/1onGzAUw4pKrySM1uS1HTeg

æœºå™¨å­¦ä¹ æ¨¡å‹å‹ç¼©ç›¸å…³æ–‡çŒ®ã€å·¥å…·ã€å­¦ä¹ èµ„æ–™åˆ—è¡¨
https://github.com/cedrickchee/awesome-ml-model-compression

æ·±åº¦ç¥ç»ç½‘ç»œä¿®å‰ª
https://towardsdatascience.com/pruning-deep-neural-network-56cae1ec5505

é¢å‘å›¾åƒåˆ†ç±»å’Œæ£€æµ‹çš„ç¥ç»ç½‘ç»œå‹ç¼©
https://arxiv.org/abs/1907.05686 https://ai.facebook.com/blog/compressing-neural-networks-for-image-classification-and-detection/

Partial Channel Connections for Memory-Efficient Differentiable Architecture Search
https://github.com/yuhuixu1993/PC-DARTS

Code for: "And the bit goes down: Revisiting the quantization of neural networks"
https://github.com/facebookresearch/kill-the-bits

Implementation with latest PyTorch for multi-gpu DARTS https://arxiv.org/abs/1806.09055
https://github.com/alphadl/darts.pytorch1.1 https://github.com/quark0/darts

ç¥ç»ç½‘ç»œæ¶æ„æœç´¢ç›¸å…³èµ„æºå¤§åˆ—è¡¨
https://github.com/D-X-Y/awesome-NAS

ã€æ¨¡å‹æ€§èƒ½å·¥å…·åº“ï¼Œä¸ºè®­ç»ƒå¥½çš„ç¥ç»ç½‘ç»œæ¨¡å‹æä¾›é«˜çº§é‡åŒ–å’Œå‹ç¼©æŠ€æœ¯ã€‘
https://github.com/quic/aimet

ã€ç”¨PyTorch&AutoTorchå®ç°çš„RegNetç¥ç»ç½‘ç»œæ¶æ„æœç´¢ã€‘
https://github.com/zhanghang1989/RegNet-Search-PyTorch

ã€ç¥ç»ç½‘ç»œçš„ä¿®å‰ªã€‘ã€ŠNeural Network Pruningã€‹
https://nathanhubens.github.io/posts/deep%20learning/2020/05/22/pruning.html

ã€æ·±åº¦å­¦ä¹ è¶…å‚ç®¡ç†å™¨
https://github.com/megvii-research/hpman

ã€æ¨¡å‹ä¼˜åŒ–åŸºç¡€ã€‘ã€ŠModel Optimization 101ã€‹
https://docs.google.com/presentation/d/1tCbwcls4c_Imx0tC3kOW3yINSIbcHuKgcHimCAT3B0Q/edit#slide=id.p

ã€PyTorchç¥ç»ç½‘ç»œå‹ç¼©æ¡†æ¶ã€‘
https://github.com/openvinotoolkit/nncf_pytorch

QToolï¼šç¥ç»ç½‘ç»œé‡åŒ–å·¥å…·ç®±
https://github.com/blueardour/model-quantization

Group Sparsity: The Hinge Between Filter Pruning and Decomposition for Network Compression. CVPR2020.
https://github.com/ofsoundof/group_sparsity

Lossless CNN Channel Pruning via Gradient Resetting and Convolutional Re-parameterization
https://github.com/DingXiaoH/ResRep

èµ°é©¬è§‚èŠ±AutoML - çŸ¥ä¹
https://zhuanlan.zhihu.com/p/212512984

æ¨¡å‹é‡åŒ–è®ºæ–‡/æ–‡æ¡£/ä»£ç åˆ—è¡¨
https://github.com/htqin/awesome-model-quantization

### Permute, Quantize, and Fine-tune: Efficient Compression of Neural Networks
https://github.com/uber-research/permute-quantize-finetune 

Memory Optimization for Deep Networks
https://github.com/utsaslab/MONeT

å¿’ä¿®æ–¯ä¹‹èˆ¹å¯å‘ä¸‹çš„çŸ¥è¯†è’¸é¦æ–°æ€è·¯
https://weibo.com/ttarticle/p/show?id=2309404569773894664467#_0

IntelÂ® Low Precision Optimization Toolï¼šIntelä½ç²¾åº¦ä¼˜åŒ–å·¥å…·
https://github.com/intel/lpot

### micronetï¼šæ·±åº¦ç½‘ç»œæ¨¡å‹å‹ç¼©ä¸éƒ¨ç½²åº“
https://github.com/666DZY666/micronet

Using ideas from product quantization for state-of-the-art neural network compression.
https://github.com/uber-research/permute-quantize-finetune

A GPU algorithm for sparse matrix-matrix multiplication
https://github.com/oresths/tSparse

Parallel Hyperparameter Optimization in Python
https://github.com/ARM-software/mango

source code of the paper: Robust Quantization: One Model to Rule Them All
https://github.com/moranshkolnik/RobustQuantization

Code and checkpoints of compressed networks for the paper titled "HYDRA: Pruning Adversarially Robust Neural Networks" (NeurIPS 2020)
https://github.com/inspire-group/hydra

GPU implementation of Xnor network on inference level.
https://github.com/metcan/Binary-Convolutional-Neural-Network-Inference-on-GPU

### Towards Optimal Structured CNN Pruning via Generative Adversarial Learning(GAL)
https://github.com/ShaohuiLin/GAL

TensorRT implementation of "RepVGG: Making VGG-style ConvNets Great Again"
https://github.com/upczww/TensorRT-RepVGG

Pushing the Limit of Post-Training Quantization by Block Reconstruction
https://github.com/yhhhli/BRECQ

Sparsifyï¼šæ˜“ç”¨çš„autoMLç¥ç»ç½‘ç»œç¨€ç–åŒ–ä¼˜åŒ–æ¥å£
https://github.com/neuralmagic/sparsify

### DeepSparse Engineï¼šç”¨ç¨€ç–åŒ–æ¨¡å‹æä¾›å‰æ‰€æœªæœ‰æ€§èƒ½çš„CPUæ¨ç†å¼•æ“
https://github.com/neuralmagic/deepsparse

ç¥ç»ç½‘ç»œé‡åŒ–/ä½ä½å®šç‚¹è®­ç»ƒç¡¬ä»¶å‹å¥½ç®—æ³•è®¾è®¡ç›¸å…³èµ„æ–™é›†
https://github.com/A-suozhang/awesome-quantization-and-fixed-point-training

â€œæœºå™¨å­¦ä¹ ä¼˜åŒ–â€è¯¾ç¨‹èµ„æ–™
github.com/rishabhk108/AdvancedOptML

ã€Š BNN - BN = ? Training Binary Neural Networks without Batch Normalizationã€‹(CVPRW 2021) 
github.com/VITA-Group/BNN_NoBN

ã€ŠAngularGrad: A New Optimization Technique for Angular Convergence of Convolutional Neural Networksã€‹(2021) 
github.com/mhaut/AngularGrad

Efficient Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster, and Better
https://www.arxiv-vanity.com/papers/2106.08962

Forward æ·±åº¦å­¦ä¹ æ¨ç†åŠ é€Ÿæ¡†æ¶ - A library for high performance deep learning inference on NVIDIA GPUs.
github.com/Tencent/Forward

Tritonï¼šè®©æ²¡æœ‰CUDAç»éªŒçš„ç ”ç©¶äººå‘˜ç¼–å†™é«˜æ•ˆçš„GPUä»£ç 
github.com/openai/triton

mmsegmentation-distillerï¼šåŸºäºmmsegmentationçš„çŸ¥è¯†è’¸é¦å·¥å…·ç®±
github.com/pppppM/mmsegmentation-distiller

TorchDistillerï¼šçŸ¥è¯†è’¸é¦å¼€æºPyTorchä»£ç é›†ï¼Œç‰¹åˆ«é¢å‘æ„ŸçŸ¥ä»»åŠ¡ï¼ŒåŒ…æ‹¬è¯­ä¹‰åˆ†å‰²ã€æ·±åº¦ä¼°è®¡ã€ç›®æ ‡æ£€æµ‹å’Œå®ä¾‹åˆ†å‰²
github.com/irfanICMLL/TorchDistiller

ç”¨NVIDIAå¼€æºæ¨¡å—å®ç°åŠ é€ŸSE(3)-Transformerè®­ç»ƒï¼Œâ€œä½¿ç”¨å†…å­˜å°‘9xï¼Œé€Ÿåº¦æ¯”åŸºå‡†å®˜æ–¹å®ç°å¿«21xâ€
github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/DrugDiscovery/SE3Transformer

é¢å‘è®¡ç®—æœºè§†è§‰çŸ¥è¯†è’¸é¦æ–‡çŒ®é›†
github.com/lilujunai/Awesome-Knowledge-Distillation-for-CV

### DS-Net++: Dynamic Weight Slicing for Efficient Inference in CNNs and Transformers
github.com/changlin31/DS-Net

PP-LCNet: A Lightweight CPU Convolutional Neural Network
https://arxiv.org/abs/2109.15099

TinyNeuralNetworkï¼šé«˜æ•ˆã€æ˜“ç”¨çš„æ·±åº¦å­¦ä¹ æ¨¡å‹å‹ç¼©æ¡†æ¶ï¼ŒåŒ…å«æ¨¡å‹ç»“æ„æœç´¢ã€å‰ªæã€é‡åŒ–ã€æ¨¡å‹è½¬æ¢ç­‰åŠŸèƒ½
github.com/alibaba/TinyNeuralNetwork

Model Compression Research Packageï¼šç”¨äºç ”ç©¶ç¥ç»ç½‘ç»œå‹ç¼©å’ŒåŠ é€Ÿæ–¹æ³•çš„åº“
github.com/IntelLabs/Model-Compression-Research-Package 

åˆ©ç”¨æ•°æ®é›†è’¸é¦æ›´é«˜æ•ˆè®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹
https://ai.googleblog.com/2021/12/training-machine-learning-models-more.html

IntelÂ® Neural Compressorï¼šè¿è¡Œåœ¨Intel CPU/GPUä¸Šçš„ç¥ç»ç½‘ç»œå‹ç¼©åº“
github.com/intel/neural-compressor 

pytorchè‡ªåŠ¨åŒ–æ¨¡å‹å‹ç¼©å·¥å…·åº“ - é’ˆå¯¹pytorchæ¨¡å‹çš„è‡ªåŠ¨åŒ–æ¨¡å‹ç»“æ„åˆ†æå’Œä¿®æ”¹å·¥å…·é›†ï¼ŒåŒ…å«è‡ªåŠ¨åˆ†ææ¨¡å‹ç»“æ„çš„æ¨¡å‹å‹ç¼©ç®—æ³•åº“
github.com/THU-MIG/torch-model-compression

### YOLOv5-Compression - YOLOv5 Series Multi-backbone(TPH-YOLOv5, Ghostnet, ShuffleNetv2, Mobilenetv3Small, EfficientNetLite, PP-LCNet, SwinTransformer YOLO), Module(CBAM, DCN), Pruning (EagleEye, Network Slimming) and Quantization (MQBench) Compression Tool Box.
github.com/Gumpest/YOLOv5-Multibackbone-Compression

OpenDeltaï¼šå‚æ•°é«˜æ•ˆè°ƒä¼˜å¼€æºæ¡†æ¶
github.com/thunlp/OpenDelta 

NNCFï¼šç¥ç»ç½‘ç»œå‹ç¼©æ¡†æ¶
github.com/openvinotoolkit/nncf

scs4onnxï¼šONNXæ¨¡å‹å‹ç¼©å·¥å…·
github.com/PINTO0309/scs4onnx

### é«˜æ•ˆæ·±åº¦å­¦ä¹ ï¼šæ·±åº¦å­¦ä¹ è¿‡ç¨‹åŠ é€ŸæŠ€å·§é›†
github.com/Mountchicken/Efficient-Deep-Learning

ATOM - Automated Tool for Optimized Modelling - A Python package for fast exploration of machine learning pipelinesâ€™
github.com/tvdboom/ATOM 

ã€æ¨¡å‹é‡åŒ–è®ºæ–‡/æ–‡æ¡£/ä»£ç åˆ—è¡¨ã€‘'Awesome Model Quantization - A list of papers, docs, codes about model quantization' by Haotong Qin GitHub: github.com/htqin/awesome-model-quantization 

ã€Model Compression Toolkit (MCT)ï¼šæ¨¡å‹å‹ç¼©å·¥å…·åŒ…ï¼Œç”¨äºåœ¨å—é™ç¡¬ä»¶ä¸‹é«˜æ•ˆä¼˜åŒ–ç¥ç»ç½‘ç»œæ¨¡å‹ã€‘â€™Model Compression Toolkit (MCT) - Model Compression Toolkit (MCT) is an open source project for neural network model optimization under efficient, constrained hardware.' by Sony GitHub: github.com/sony/model_optimization

ã€Awesome AutoDLï¼šæ·±åº¦å­¦ä¹ è‡ªåŠ¨åŒ–(ç¥ç»æ¶æ„æœç´¢å’Œè¶…å‚æ•°è‡ªåŠ¨ä¼˜åŒ–)ç›¸å…³èµ„æºå¤§åˆ—è¡¨ã€‘â€™Awesome AutoDL - A curated list of automated deep learning (including neural architecture search and hyper-parameter optimization) resources.' by D-X-Y GitHub: github.com/D-X-Y/Awesome-AutoDL

### ã€VoltaMLï¼šç”¨äºåŠ é€Ÿæœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æ¨¡å‹çš„è½»é‡å¼€æºåº“ï¼Œå¯ä¼˜åŒ–ã€ç¼–è¯‘å’Œéƒ¨ç½²æ¨¡å‹åˆ°ç›®æ ‡CPUå’ŒGPUè®¾å¤‡ï¼Œåªéœ€ä¸€è¡Œä»£ç ã€‘â€™Accelerating Huggingface Models using voltaML - VoltaML is a lightweight library to convert and run your ML/DL deep learning models in high performance inference runtimes like TensorRT, TorchScript, ONNX and TVM.' by VoltaML GitHub: github.com/VoltaML/voltaML

ã€ä¸€è¡Œä»£ç æé«˜Hugging Face Transformersæ€§èƒ½ã€‘ã€ŠBetterTransformer, Out of the Box Performance for Hugging Face Transformersã€‹by Younes Belkada medium.com/pytorch/bettertransformer-out-of-the-box-performance-for-huggingface-transformers-3fbe27d50ab2

ã€voltaML-fast-stable-diffusionï¼šä¸€è¡Œä»£ç åŠ é€ŸStable Diffusion(10x)çš„è½»é‡åº“ã€‘'voltaML-fast-stable-diffusion - Lightweight library to accelerate Stable-Diffusion, Dreambooth into fastest inference models with single line of code ğŸ”¥ ğŸ”¥' by VoltaML GitHub: github.com/VoltaML/voltaML-fast-stable-diffusion

ã€Intelå¹³å°åŠ é€Ÿç‰ˆHugging Face transformersæ‰©å±•å·¥å…·åŒ…ï¼Œåˆ©ç”¨Intelç¥ç»å‹ç¼©å™¨æä¾›çš„ä¸€å¥—ä¸°å¯Œçš„æ¨¡å‹å‹ç¼©æŠ€æœ¯: é‡åŒ–ã€å‰ªæã€è’¸é¦ç­‰ï¼Œæ˜¾è‘—æé«˜äº†è‹±ç‰¹å°”å¹³å°ä¸Šçš„æ¨ç†æ•ˆç‡ã€‘â€™IntelÂ® Extension for Transformers: Accelerating Transformer-based Models on Intel Platforms' by Intel GitHub: github.com/intel/intel-extension-for-transformers

ã€å¤§å‹ Transformer æ¨¡å‹æ¨ç†ä¼˜åŒ–ã€‘ã€ŠLarge Transformer Model Inference Optimization | Lil'Logã€‹ 
https://lilianweng.github.io/posts/2023-01-10-inference-optimization/

'mperf - é¢å‘ç§»åŠ¨/åµŒå…¥å¼å¹³å°çš„ç®—å­æ€§èƒ½è°ƒä¼˜å·¥å…·ç®±' MegEngine GitHub: github.com/MegEngine/mperf

ã€Dipoorletï¼šç¦»çº¿é‡åŒ–å·¥å…·ï¼Œå¯ä»¥å¯¹ç»™å®šæ ¡å‡†æ•°æ®é›†ä¸Šçš„ONNXæ¨¡å‹è¿›è¡Œç¦»çº¿é‡åŒ–ã€‚æ”¯æŒå¤šç§æ¿€æ´»æ ¡å‡†ç®—æ³•ï¼Œå¦‚Mseã€Minmaxã€Histç­‰ï¼›æ”¯æŒæƒé‡è½¬æ¢ä»¥è·å¾—æ›´å¥½çš„é‡åŒ–ç»“æœï¼Œå¦‚BiasCorrectionã€WeightEqualizationç­‰ï¼›æ”¯æŒæœ€æ–°çš„ç¦»çº¿å¾®è°ƒç®—æ³•ä»¥æé«˜é‡åŒ–æ€§èƒ½ï¼Œå¦‚Adaroundã€Brecqã€Qdropã€‚æ­¤å¤–ï¼ŒDipoorletèƒ½ç”Ÿæˆå¤šä¸ªå¹³å°æ‰€éœ€çš„é‡åŒ–å‚æ•°ï¼Œå¹¶æä¾›è¯¦ç»†çš„é‡åŒ–åˆ†æä»¥å¸®åŠ©ç”¨æˆ·è¯†åˆ«æ¨¡å‹é‡åŒ–ä¸­çš„å‡†ç¡®æ€§ç“¶é¢ˆã€‚å®‰è£…å’Œä½¿ç”¨éƒ½å¾ˆç®€å•ï¼Œç”¨æˆ·éœ€è¦å‡†å¤‡æ ¡å‡†æ•°æ®é›†ï¼Œå¹¶åœ¨Pytorchåˆ†å¸ƒå¼ç¯å¢ƒæˆ–é›†ç¾¤ç¯å¢ƒä¸­è¿è¡ŒDipoorletã€‘'Dipoorlet - Offline Quantization Tools for Deploy.' ModelTC GitHub: github.com/ModelTC/Dipoorlet

ã€Autodistillï¼šä¸€ç§åˆ©ç”¨åŸºç¡€æ¨¡å‹è®­ç»ƒç›‘ç£æ¨¡å‹çš„æ–¹æ³•ï¼Œå¯ä»¥åœ¨æ²¡æœ‰æ ‡ç­¾çš„å›¾åƒä¸Šè¿›è¡Œæ¨ç†ã€‚é€šè¿‡è‡ªåŠ¨è’¸é¦ï¼Œå¯ä»¥å®ç°ä»æ— æ ‡ç­¾å›¾åƒåˆ°åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šè¿›è¡Œæ¨ç†çš„è‡ªå®šä¹‰æ¨¡å‹ï¼Œå®Œå…¨æ— éœ€äººå·¥å¹²é¢„ã€‚ç›®å‰ï¼Œè‡ªåŠ¨è’¸é¦æ”¯æŒç›®æ ‡æ£€æµ‹å’Œå®ä¾‹åˆ†å‰²ç­‰è§†è§‰ä»»åŠ¡ï¼Œæœªæ¥è¿˜å¯ä»¥æ‰©å±•åˆ°æ”¯æŒè¯­è¨€ç­‰å…¶ä»–æ¨¡å‹ã€‘'Autodistill - Images to inference with no labeling (use foundation models to train supervised models)' GitHub: github.com/autodistill/autodistill

ã€InfiniTensorï¼šæ·±åº¦å­¦ä¹ é¢†åŸŸçš„ç¼–è¯‘å™¨é›†åˆï¼Œæ—¨åœ¨ç¼©å°æ·±åº¦å­¦ä¹ åº”ç”¨ä¸åç«¯ç¡¬ä»¶ä¹‹é—´çš„é¸¿æ²Ÿï¼Œé€šè¿‡ä½¿ç”¨ç¼–è¯‘å™¨è¶…ä¼˜åŒ–æŠ€æœ¯ï¼Œå¯¹ç¥ç»ç½‘ç»œæ¨¡å‹è¿›è¡Œä¼˜åŒ–ï¼Œä»è€Œè·å¾—æ›´å¥½çš„æ€§èƒ½ã€‘â€™InfiniTensor' GitHub: github.com/InfiniTensor/InfiniTensor

ã€OpenVINOâ„¢ Plugins for OBS Studioï¼šé€‚ç”¨äºOBS Studioçš„OpenVINOâ„¢æ’ä»¶ï¼Œç›®å‰æä¾›ä»¥ä¸‹æ’ä»¶ï¼šæ™ºèƒ½å‰ªè£(æ£€æµ‹åœºæ™¯ä¸­çš„äººä½“/äººè„¸ï¼Œè‡ªåŠ¨è£å‰ª/å±…ä¸­/è°ƒæ•´å¤§å°ä»¥å›´ç»•æ„Ÿå…´è¶£çš„ä¸»ä½“)ã€èƒŒæ™¯éšè—(å°†å‰æ™¯ä¸­çš„äººä¸èƒŒæ™¯åˆ†ç¦»ï¼Œé€šè¿‡æ¨¡ç³Šã€é™æ€å›¾åƒæˆ–è™šæ‹Ÿç»¿å¹•(ç”¨äºç»¿å¹•é”®æ§)åˆ é™¤èƒŒæ™¯)ã€äººè„¸ç½‘æ ¼(ä½¿ç”¨OpenVINOå’ŒOpenCVæ‰§è¡ŒMediapipeçš„äººè„¸ç½‘æ ¼æµæ°´çº¿ï¼Œåœ¨ç»“æœå¸§ä¸Šå åŠ 468ä¸ªäººè„¸å…³é”®ç‚¹)ã€‘â€™OpenVINOâ„¢ Plugins for OBS Studio' by Intel GitHub: github.com/intel/openvino-plugins-for-obs-studio

ã€Mobile GPUå·¥ä½œåŸç†è§£æ(æ¼«ç”»ç‰ˆ)ã€‘
https://armkeil.blob.core.windows.net/developer/Files/pdf/graphics-and-multimedia/how-does-a-mobile-gpu-work.pdf