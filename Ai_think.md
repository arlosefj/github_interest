è‘—åé£æŠ•Andreessen-Horowitzå¯¹â€œäººå·¥æ™ºèƒ½â€åˆåˆ›ä¼ä¸šçš„é«˜åº¦ç‚¹è¯„ï¼šäº‘åŸºç¡€è®¾æ–½æ˜¯AIå…¬å¸çš„é‡è¦éšæ€§æˆæœ¬ï¼Œè¦è·å¾—è¾¹é™…æ”¶ç›Šï¼Œéœ€è¦ä»˜å‡ºå¾ˆé«˜çš„æ·±åº¦å­¦ä¹ è®¡ç®—æˆæœ¬ï¼›è®¸å¤šAIåº”ç”¨å¯¹äººå·¥ä¾èµ–ç¨‹åº¦å¾ˆé«˜ï¼›åˆåˆ›å…¬å¸é€šå¸¸æ²¡ä»€ä¹ˆæŠ¤åŸæ²³æˆ–ç§˜å¯†æ­¦å™¨ï¼›åˆ›ä¸šå…¬å¸ä¸»è¦æä¾›æœåŠ¡ï¼Œè€Œä¸æ˜¯è½¯ä»¶ä¸šåŠ¡ï¼›æœºå™¨å­¦ä¹ åœ¨æ•°æ®å’Œæµç¨‹æ•ˆç‡ä½ä¸‹çš„å¤§å‹ç»„ç»‡ä¸­æœ€æœ‰ç”Ÿäº§åŠ›
https://scottlocklin.wordpress.com/2020/02/21/andreessen-horowitz-craps-on-ai-startups-from-a-great-height/

ä¸è¦æŠŠäººç±»çœ‹ä½œæ˜¯ä¸‡ç‰©çš„ç‹å† ã€‚ ç›¸åï¼ŒæŠŠäººç±»æ–‡æ˜çœ‹ä½œæ˜¯ä¸€ä¸ªæ›´å®å¤§è®¡åˆ’çš„ä¸€éƒ¨åˆ†ï¼Œæ˜¯å®‡å®™èµ°å‘æ›´é«˜å¤æ‚æ€§é“è·¯ä¸Šçš„é‡è¦ä¸€æ­¥ã€‚

Nikhil Thoratï¼šè½¯ä»¶å·¥ç¨‹å¸ˆå’Œç ”ç©¶äººå‘˜å–œæ¬¢ç®—æ³•é—®é¢˜(åŒ…æ‹¬æ·±åº¦å­¦ä¹ åº“) ï¼Œå› ä¸ºè¿™äº›é—®é¢˜å¹²å‡€åˆçº¯ç²¹ãƒ¼ãƒ¼ä½†ç°å®ä¸–ç•Œçš„æ•°æ®å¹¶éå¦‚æ­¤ã€‚è¿™ä¸ºäººä»¬æ„å»ºå·¥å…·æ¥ç†è§£å’Œå¤„ç†è„æ•°æ®ç•™å‡ºäº†å·¨å¤§çš„æœºä¼šã€‚ â€‹â€‹â€‹â€‹

FranÃ§ois Cholletï¼šä¸€ç§æè¿°æ™ºèƒ½çš„æ–¹æ³•ï¼Œæ˜¯â€œä»ç‰‡é¢æè¿°ä¸­ç†è§£äº‹ç‰©çš„èƒ½åŠ›â€ã€‚è¿™ç§ç®€å•çš„æè¿°ï¼Œå¯ä»¥ä½œä¸ºæ™ºèƒ½çš„ä¸€ä¸ªè¯„ä»·æ ‡å‡†ã€‚å¦‚æœä½ çš„AIï¼Œéœ€è¦æå…¶è¯¦å°½çš„ä»»åŠ¡æè¿°(ç”¨æ˜ç¡®çš„æ‰‹å·¥ç¼–å†™çš„ç¨‹åºï¼Œæˆ–ç”¨é™æ€æ•°æ®é›†å¯†é›†é‡‡æ ·) ï¼Œå®ƒå®é™…ä¸Šå¹¶æ˜¯ä¸æ™ºèƒ½çš„ã€‚å½“ä½ æ“ä½œå®ƒçš„æ—¶å€™ï¼Œä½ åªæ˜¯åœ¨æŸ¥è¯¢ä½ è¾“å…¥è¿‡çš„ä¿¡æ¯ï¼Œè€Œéå®ƒçš„è‡ªä¸»ååº”ã€‚æ™ºèƒ½æ˜¯AIå°†ä»»åŠ¡ä¿¡æ¯è½¬åŒ–ä¸ºä»»åŠ¡æŠ€èƒ½çš„æ¯”ç‡ã€‚é‡è¦çš„æ˜¯ï¼Œä»»åŠ¡é€šå¸¸ä¸æ˜¯é™æ€çš„ï¼Œæ‰€ä»¥è¿™é‡Œçš„â€œæŠ€èƒ½â€é’ˆå¯¹çš„æ˜¯æœªæ¥çš„æƒ…å†µâ€”â€”å¯èƒ½å’Œè¿‡å»çš„æƒ…å†µæˆªç„¶ä¸åŒã€‚è½¬åŒ–ï¼Œæ˜¯ä»è¿‡å»çš„æ“ä½œç©ºé—´åˆ°æœªæ¥çš„æ“ä½œç©ºé—´ã€‚

â€œå¤ä»Šä¹‹æˆå¤§äº‹ä¸šã€å¤§å­¦é—®è€…ï¼Œå¿…ç»è¿‡ä¸‰ç§ä¹‹å¢ƒç•Œï¼šâ€˜æ˜¨å¤œè¥¿é£å‡‹ç¢§æ ‘ï¼Œç‹¬ä¸Šé«˜æ¥¼ï¼Œæœ›å°½å¤©æ¶¯è·¯â€™ã€‚æ­¤ç¬¬ä¸€å¢ƒä¹Ÿã€‚â€˜è¡£å¸¦æ¸å®½ç»ˆä¸æ‚”ï¼Œä¸ºä¼Šæ¶ˆå¾—äººæ†”æ‚´ã€‚â€™æ­¤ç¬¬äºŒå¢ƒä¹Ÿã€‚â€˜ä¼—é‡Œå¯»ä»–åƒç™¾åº¦ï¼Œè“¦ç„¶å›é¦–ï¼Œé‚£äººå´åœ¨ç¯ç«é˜‘çŠå¤„â€™ã€‚æ­¤ç¬¬ä¸‰å¢ƒä¹Ÿã€‚â€ - ç‹å›½ç»´ã€Šäººé—´è¯è¯ã€‹ â€‹â€‹â€‹â€‹
æƒ³åšå¤§äº‹ï¼Œé¦–å…ˆè¦è·³å‡ºçº·ç¹çš„ä¸–ç•Œï¼Œç»å†å­¤ç‹¬çš„æ€è€ƒï¼Œçœ‹åˆ°çœŸæ­£æƒ³è¦çš„ä¸œè¥¿â€”â€”è·³å‡ºçº·æ‰°ï¼Œæ‰¾åˆ°ç›®æ ‡ï¼›è‹¦è‹¦æ±‚ç´¢ï¼Œåå¤æ‘¹æƒ³ï¼Œä»˜å‡ºä¸€åˆ‡ä¹Ÿç»ä¸åæ‚”ï¼Œä¸ä¼šä¸ºçŸ­æœŸå¾—å¤±æ–¤æ–¤è®¡è¾ƒï¼Œä¹Ÿä¸ä¼šä¸ºæœ€ç»ˆæ— æœè€Œæ‚”æ¨â€”â€”å­œå­œä¸å€¦ï¼Œæ— æ‚”è¿½å¯»ï¼›æœ€å¥½çš„ç»“æœï¼Œæ˜¯åœ¨å†ç»å¯»æ‰¾ã€å†ç»æ€è€ƒä»¥åçš„é¡¿æ‚Ÿã€‚

ã€æ•°æ®ä¹‹æƒ‘ï¼šOne-Shot & Zero-Shot Learningã€‘ã€ŠThe Data Problem II: One-Shot and Zero-Shot Learning | Synthesis AIã€‹
https://synthesis.ai/2020/03/30/the-data-problem-ii-one-shot-and-zero-shot-learning/

ã€æ•°æ®ä¹‹æƒ‘ï¼šæœªæ ‡æ³¨æ•°æ®æœ‰ç”¨å—ï¼Ÿã€‘ã€ŠThe Data Problem IV: Can Unlabeled Data Help? | Synthesis AIã€‹
https://synthesis.ai/2020/04/14/the-data-problem-iv-can-unlabeled-data-help/


FranÃ§ois Cholletï¼šå“²å­¦ç»å¸¸å—åˆ°ç§‘æŠ€ç•Œäººå£«çš„å˜²ç¬‘ã€‚ä½†æˆ‘å¾ˆå–œæ¬¢ã€‚æˆ‘æ‰€åšè¿‡æœ€æœ‰ä»·å€¼çš„äº‹ï¼Œéƒ½å’Œâ€œå“²å­¦â€å¤§æœ‰æ¸Šæºã€‚ å“²å­¦å¼ºå¤§ã€å¯è¡Œã€å®ç”¨ï¼Œåˆä¸ä¼—ä¸åŒã€‚å“²å­¦çš„çœŸæ­£ç›®çš„ï¼Œä¸æ˜¯å¾—å‡ºå…·ä½“çš„ç†è®º(ç†è®ºåæ˜ çš„ï¼Œå¾€å¾€æ—¶æ‰€å¤„æ—¶ä»£çš„å±€é™) ï¼Œæˆ–æ•™ç»™ä½ æœ‰ç”¨çš„â€œç”Ÿæ´»æŠ€å·§â€ã€‚ å“²å­¦ä¸æ˜¯ç§‘å­¦ï¼Œä¹Ÿä¸æ˜¯å·¥ç¨‹ã€‚ å“²å­¦çš„ç›®çš„ï¼Œæ˜¯ä¸ºä½ æä¾›æ€è€ƒçš„åŸºçŸ³ã€‚è¯»å“²å­¦çš„ä»·å€¼ï¼Œä¸åœ¨äºä½ èƒ½è®°ä½ä»€ä¹ˆæ¦‚å¿µæˆ–ç†è®ºï¼Œè€Œåœ¨äºå®ƒä»¬å¯¹ä½ æœªæ¥æ€ç»´æ–¹å¼çš„æ‹“å±•ã€‚å°±åƒå­¦ç¼–ç¨‹çš„ä»·å€¼ï¼Œä¸åœ¨äºä½ èƒ½å†™å‡ºä»€ä¹ˆæ ·çš„å°ç¨‹åºï¼Œè€Œåœ¨äºå¯¹ä½ æ€ç»´æ–¹å¼çš„åˆ·æ–°ã€‚

FranÃ§ois Cholletï¼šæœºå™¨å­¦ä¹ æœ€æœ‰è¶£çš„åº”ç”¨ï¼Œå¾€å¾€æ¥è‡ªäºæ²¡å¤šå°‘æŠ€æœ¯èƒŒæ™¯çš„äººã€‚ ç§‘æŠ€ä¸è‰ºæœ¯çš„äº¤å‰ï¼Œæ˜¯äººæ€§æœ€èƒ½é—ªå…‰çš„åœ°æ–¹ã€‚ â€‹â€‹â€‹â€‹

çˆ±å› æ–¯å¦ï¼šç¾åœ¨æœ¬è´¨ä¸Šç»ˆç©¶æ˜¯ç®€å•æ€§ã€‚

Jacob Menickï¼šä¸€æ—¦ä½ æ„è¯†åˆ°ï¼Œæœºå™¨å­¦ä¹ æ¨¡å‹åªæ˜¯æ•°æ®çš„ä¸€ä¸ªè§†å›¾ï¼Œä½ å°±ä¼šçœŸæ­£å¼€å§‹å¯¹æ•°æ®é›†æ„Ÿå…´è¶£ã€‚ 

å¦‚ä½•åšç§‘ç ”ï¼Ÿ
â€¢ æ²‰è¿·äºå‘ç°çš„å¿«ä¹
â€¢ æ‹¥æŠ±ä¸ç¡®å®šæ€§ï¼Œå‹‡äºè´¨ç–‘
â€¢ æŠŠæ¯ä¸€ä¸ªé—®é¢˜ï¼Œå½“ä½œä¸€æ¬¡æœºä¼š
â€¢ å¯¹æ‰€æœ‰äº‹ç‰©ä¿æŒå¥½å¥‡
â€¢ çƒ­çˆ±ç»†èŠ‚
â€¢ ä¸æ–­æ€è€ƒ
â€¢ æ·±åº¦ä¸“æ³¨
â€¢ æ‹’ç»â€œå¤©æ‰â€è¯´

å¤šè¯»è®ºæ–‡å¤šæ€è€ƒã€‚æ„Ÿè§‰è¿·èŒ«ã€æ²¡æƒ³æ³•ï¼Œè¦ä¹ˆæ˜¯æ–‡çŒ®è¯»çš„ä¸å¤Ÿå¤šï¼Œè¦ä¹ˆæ˜¯æ²¡æœ‰çœŸæ­£å»æ€è€ƒï¼Œæˆ–è§’åº¦ä¸å¯¹ã€‚åˆ›æ–°ï¼Œä¸€å®šæ˜¯åœ¨å……åˆ†äº†è§£çš„åŸºç¡€ä¸Šâ€”â€”ä¸åªçœ‹åˆ«äººåšäº†ä»€ä¹ˆï¼Œè¦çœ‹åˆ«äººæ˜¯æ€ä¹ˆæ‰¾é—®é¢˜ã€æ‰¾è§’åº¦çš„ï¼Œè®¤çœŸåšä¸€æ¬¡æ–‡çŒ®ç»¼è¿°ï¼Œä¸€å®šä¼šæœ‰æ‰€æ”¶è·ã€‚é—®é¢˜å¯¹äº†ï¼Œå°±å·²ç»æˆåŠŸäº†ä¸€åŠã€‚

FranÃ§ois Cholletï¼šæ¨¡å‹æ“…é•¿é¢„æµ‹è¿‡å»ï¼Œè€Œæœªæ¥æ‰æ˜¯ä»–ä»¬è¦å»é¢å¯¹çš„ã€‚

ä»¥å‰ä¸Yann LeCunèŠå¤©ï¼Œä»–è°ˆåˆ°ç¥ç»ç½‘ç»œä¸ºä»€ä¹ˆç°ä»Šå¦‚æ­¤æµè¡Œçš„åŸå› ï¼Œæ˜¯å› ä¸ºæ‰€æœ‰é‡è¦çš„ideaséƒ½æ˜¯ä¸‰åå¹´å‰çš„ï¼Œå·²ç»æ²¡æœ‰ä»»ä½•æ–°çš„ideaså¯ä»¥è¢«patentï¼Œå¤§å®¶éƒ½å¯ä»¥ç”¨ã€‚è€Œè¿™ä¸€è½®æ·±åº¦å­¦ä¹ çš„ä¸»è¦è´¡çŒ®æ˜¯å¯¹è¿™äº›ideasçš„é›†æˆå’Œå®ç°ã€‚å½“ç„¶ï¼Œç•™ç»™æˆ‘ä»¬åšç†è®ºçš„å°±æ˜¯è§£é‡Šã€ç®€åŒ–å’Œç»Ÿä¸€ï¼

ã€Marcusè®¿è°ˆï¼šAIçš„å¸Œæœ›ã€éšæ‚£ä¸æœªæ¥ã€‘
https://jamanetwork.com/journals/jama/article-abstract/2766942

Sam Altmanï¼šAIå·¥å…·ä¼šå¹²æ‰å¾ˆå¤šå·¥ä½œ(è®¾è®¡ã€æ•°æ®ç§‘å­¦ã€ç¼–ç¨‹ã€ä¸šåŠ¡é‚®ä»¶å¤„ç†ç­‰)æœ€æ¯ç‡¥ä¹å‘³çš„éƒ¨åˆ†ã€‚å¯ä»¥é¢„è§ï¼šå½“æˆ‘ä»¬å†ä¹Ÿä¸ç”¨ç ´åèŠ‚å¥ã€æ…¢ä¸‹æ¥å¹²é‚£äº›é‡å¤ã€ä¹å‘³çš„ä»»åŠ¡ï¼Œå·¥ä½œæ•ˆç‡å°†ä¼šå¤§å¤§æé«˜ï¼Œè¿™ç§å˜åŒ–å°†æ˜¯éå¸¸æƒŠäººçš„ã€‚ â€‹â€‹â€‹â€‹

@æ¯…é©¬å½“é—² æœ€è¿‘è¶Šæ¥è¶Šå¤šçš„è¯æ®è¡¨æ˜ï¼Œé€šè¿‡æ‹Ÿåˆæ•°æ®å¾—åˆ°çš„æ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹ï¼ˆåœ¨classificationï¼Œdetectionï¼Œsegmentationç­‰ï¼‰å¯¹è¾“å…¥å¾ˆå°çš„æ•°å€¼æ‰°åŠ¨å’Œå¾ˆå°çš„å˜æ¢deformationï¼ˆç”šè‡³å¹³ç§»ï¼‰éƒ½æ˜¯ä¸ç¨³å®šçš„ï¼ˆunstable)ï¼Œæ›´è°ˆä¸ä¸Šé²æ£’ã€‚ä¸è¦å†ç›¸ä¿¡åˆ«äººshowçš„æˆåŠŸä¾‹å­ -- æˆ‘è¿‡å»å°±æ˜¯è¢«åˆ«äººshowçš„ä¸€äº›ä¾‹å­è¿·ç³Šï¼Œæœ‰äº›ç›¸ä¿¡è¿™æ ·çš„æ¨¡å‹ï¼ˆé€šè¿‡åœ¨augmentedæ•°æ®ä¸Šè®­ç»ƒï¼‰ä¼šæ˜¯ç¨³å®šçš„ï¼Œè€Œè‡ªå·±æ²¡æœ‰å»åšä¸¥æ ¼çš„éªŒè¯ -- è‚ å­éƒ½æœ‰äº›æ‚”é’äº†ã€‚ä½†åº”è¯¥ä¸ä¼šå†è¢«å¿½æ‚ äº†ã€‚æ‰€ä»¥ç›®å‰åŸºäºæ·±åº¦å­¦ä¹ çš„â€œäººå·¥æ™ºèƒ½â€ï¼Œç”¨åœ¨ä¸ç—›ä¸ç—’çš„åº”ç”¨ä¸Šï¼Œä¹Ÿå°±ç½¢äº†ã€‚æŠŠè¿™æ ·çš„æ¨¡å‹ç”¨åœ¨ä¸¥è‚ƒçš„é—®é¢˜ä¸Šï¼ˆä¾‹å¦‚éœ€è¦æœ‰å®‰å…¨ã€éšç§ã€å¯é æ€§ä¿éšœçš„ï¼‰ï¼Œåº”è¯¥æ˜¯ååˆ†å±é™©çš„ã€‚è™½ç„¶è¿™å¹¶ä¸æ˜¯è¯´ï¼Œé€šè¿‡ç³»ç»Ÿä¸¥æ ¼çš„æ”¹è¿›ï¼Œæ·±åº¦æ¨¡å‹å’Œç®—æ³•å°±ä¸èƒ½æ²¡æœ‰æ€§èƒ½ä¸Šä¿éšœã€‚ä½†é‚£éœ€è¦å»ºç«‹ä¸€å¥—å®Œæ•´çš„ç†è®ºä½“ç³»ï¼Œæ­£ç¡®çš„æ¨¡å‹éœ€è¦æ¨å¯¼å‡ºæ¥ï¼ˆè€Œä¸æ˜¯è¯•é”™å‡ºæ¥ï¼‰ï¼Œè€Œå…¶æ€§èƒ½ä¿è¯ä¹Ÿå¿…é¡»è¦æœ‰ä¸¥æ ¼çš„è¯æ˜ã€‚å…¶å®ä¸å°‘é¡¶å°–çš„ç ”ç©¶äººå‘˜éƒ½å·²ç»æ„è¯†åˆ°è¿™ä¸€ç‚¹ï¼Œä»Šåå‡ å¹´ï¼Œå¤§å®¶åº”è¯¥ä¼šçœ‹åˆ°ç³»ç»Ÿçš„ç†è®ºç ”ç©¶çš„å¼ºåŠ¿å›å½’ã€‚ä¸ä¼šè®©æ·±åº¦å­¦ä¹ æŠŠä¼ ç»Ÿå·¥ç¨‹ç†è®ºå·²ç»å¾—åˆ°çš„å¸¸è¯†å’Œæ•™è®­å†ä»æ–°å‘æ˜ä¸€éã€‚

@æ¯…é©¬å½“é—² æœ‰è¶£çš„æ˜¯æˆ‘ä»¬æœ€è¿‘çš„å·¥ä½œå‘ç°æœºå™¨å­¦ä¹ (æ·±åº¦å­¦ä¹ )ä¸sphere-packingé—®é¢˜æœ¬è´¨ä¸Šæ˜¯ä¸€è‡´çš„ã€‚ç›®å‰çš„æœºå™¨å­¦ä¹ å°±æ˜¯æŒ‡å¯¼æœºå™¨å¦‚ä½•æŠŠæ•°æ®ç´§è‡´æœ‰æ•ˆåœ°packåˆ°é«˜ç»´ç©ºé—´ä¸­ã€‚ä¸åƒæ•°å­¦å®¶æ±‚closed-formè§£ï¼Œä½†æ±‚æœ‰æ•ˆçš„æ•°å€¼é€¼è¿‘æœ€ä¼˜è§£ã€‚å…¶å®ä¿¡æ¯ç†è®ºcoding theoryçš„åŸºç¡€ï¼Œä¹Ÿæ˜¯sphere packingã€‚ä»»ä½•å­¦é—®ï¼Œç®€å•åˆ°äº†æè‡´ï¼Œæ‰æ˜¯æœ¬è´¨ã€‚

FranÃ§ois Cholletï¼šAIçš„å‘å±•æ–¹å‘ï¼Œåº”è¯¥æ˜¯ç†è§£ï¼Œè€Œéæ¨¡ä»¿ã€‚è¦ç†è§£ç”Ÿå‘½çš„æœ¬è´¨ï¼Œå°±è¯¥çœ‹çœ‹è‡ªåˆ›ç”Ÿ(autopoiesis)ã€åˆ†å­ç”Ÿç‰©å­¦ã€DNAï¼›è¦æ¨¡ä»¿ç”Ÿå‘½ï¼Œå¯ä»¥ç”»å¡é€šç”»ã€‚ä¸¤æ¡è·¯æˆªç„¶ä¸åŒã€‚å¤§é‡çš„â€œäººå·¥æ™ºèƒ½â€ï¼Œå…¶å®å°±æ˜¯ç¥ç»å¡é€šç‰‡ã€‚èƒ½*ç†è§£*çš„äººå·¥æ™ºèƒ½ï¼Œä¸ä¼šä»ä¸€å¼€å§‹å°±çœ‹èµ·æ¥å¾ˆèªæ˜çš„æ ·å­ï¼Œä½†å®ƒä¼šé€æ¸æ‰©å±•åˆ°é‚£ä¸ªå±‚æ¬¡â€”â€”ç›´åˆ°å’Œæˆ‘ä»¬ä¸€æ ·ã€‚å¡é€šç‰‡çœ‹èµ·æ¥åƒçœŸçš„ï¼Œä½†å®é™…ä¸Šæ²¡å•¥çœŸä¸œè¥¿ã€‚å¡é€šç‰‡åªèƒ½æ”¾ä½ ç”»çš„åœºæ™¯ï¼Œä½†ç”Ÿå‘½ä½“ä¼šä»¥å¼€æ”¾çš„æ–¹å¼è‡ªé€‚åº”ï¼Œç”Ÿå‘½ä½“æ˜¯å®Œå…¨è‡ªä¸»çš„ã€‚

ã€æœ€é‡è¦çš„æ·±åº¦å­¦ä¹ æ€æƒ³â€”â€”ç®€è¦å†å²å›é¡¾(æ·±åº¦å­¦ä¹ æ¨èè®ºæ–‡åˆ—è¡¨)ã€‘
https://dennybritz.com/blog/deep-learning-most-important-ideas/

FranÃ§ois Cholletï¼šç†æƒ³ï¼šå¦‚æœæˆ‘ä»¬å‘æ˜å‡ºå¼ºäººå·¥æ™ºèƒ½ï¼Œå°±å¯ä»¥ç”¨å®ƒæ¥è§£å†³å…¶ä»–æ‰€æœ‰é—®é¢˜ã€‚ç°å®ï¼šå¦‚æœæˆ‘ä»¬ä¸èƒ½æ‰¾åˆ°è§£å†³ç—…æ¯’å¼å‡æ¶ˆæ¯çš„æ–¹æ³•ï¼Œå°±æ ¹æœ¬è§£å†³ä¸äº†ä»»ä½•é—®é¢˜ã€‚ â€‹â€‹â€‹â€‹

æœ€å¥½çš„å­¦ä¹ è€…ï¼Œæ˜¯é‚£äº›èƒ½å…‹æœå®¢è§‚é¢å¯¹ä¸æ“…é•¿æŸäº‹æ‰€äº§ç”Ÿçš„ä¸é€‚æ„Ÿçš„äººã€‚ - Tommy Collison â€‹â€‹â€‹â€‹

FranÃ§ois Cholletï¼šå…³äºæœºå™¨å­¦ä¹ ï¼Œé¦–å…ˆè¦è®°ä½ï¼Œæ€§èƒ½æ˜¯ç”¨æ•°æ®é›†é‡Œçš„æ ·æœ¬è¯„ä»·çš„ï¼Œæ¨¡å‹åœ¨ç”Ÿäº§ç¯å¢ƒé¢å¯¹çš„æ ·æœ¬ï¼Œå¾ˆå¯èƒ½å¤§ç›¸å¾„åº­â€¦â€¦å¯¹æ­¤ï¼Œé‡‘èä¸šæœ‰ç§è¯´æ³•: â€œè¿‡å»çš„è¡¨ç°å¹¶ä¸èƒ½ä¿è¯æœªæ¥çš„ç»“æœâ€ã€‚æ¨¡å‹åœ¨æµ‹è¯•æ•°æ®é›†ä¸Šå¾—åˆ°Xåˆ†ï¼Œå¹¶ä¸ä»£è¡¨æ”¾åˆ°ç°å®ä¸–ç•Œï¼Œåœ¨æ¥ä¸‹æ¥çš„Nç§æƒ…å†µä¸‹ä¹Ÿèƒ½è¾¾åˆ°Xã€‚æœªæ¥ä¸è¿‡å»ï¼Œå¯èƒ½æœ‰å¾ˆå¤§å·®å¼‚ã€‚å› æ­¤ï¼Œå½“é—®åˆ°â€œä½ æ›´æ„¿æ„ç”¨90%å‡†ç¡®ç‡çš„æ¨¡å‹ï¼Œè¿˜æ˜¯80%å‡†ç¡®ç‡çš„äººâ€æ—¶ï¼Œç­”æ¡ˆå–å†³äºç”¨æ¥è¯„ä»·çš„æ•°æ®æ˜¯å¦å¤Ÿå…¸å‹ã€‚äººæœ‰å¼ºå¤§çš„é€‚åº”èƒ½åŠ›ï¼Œè€Œæ¨¡å‹æ²¡æœ‰ã€‚å¦‚æœå­˜åœ¨é‡å¤§çš„ä¸ç¡®å®šæ€§ï¼Œå°±é€‰æ‹©äººç±»ï¼Œä»–ä»¬æ¨¡å¼è¯†åˆ«èƒ½åŠ›å¯èƒ½ä¸é‚£ä¹ˆå¼ºå¤§(ç›¸å¯¹äºç”¨å¤§é‡æ•°æ®è®­ç»ƒå‡ºæ¥çš„æ¨¡å‹)ï¼Œä½†ä»–ä»¬çŸ¥é“è‡ªå·±åœ¨åšä»€ä¹ˆï¼Œå¯ä»¥è‡ªè¡Œæ¨ç†ï¼Œé¢å¯¹æ–°å¥‡äº‹ç‰©æ—¶ï¼Œè¿˜èƒ½å³å…´å‘æŒ¥ã€‚å¦‚æœæ¯ç§å¯èƒ½çš„æƒ…å†µéƒ½å·²ç»æŒæ¡ï¼Œå¹¶ä¸”å¸Œæœ›ä¼˜å…ˆè€ƒè™‘å¯æ‰©å±•æ€§ã€é™ä½æˆæœ¬ï¼Œå°±é€‰æ‹©æ¨¡å‹ã€‚ æ¨¡å‹çš„å­˜åœ¨ï¼Œå°±æ˜¯ä¸ºäº†åœ¨ç†è§£é€å½»çš„æƒ…å†µä¸‹ï¼Œå¯¹äººç±»è®¤çŸ¥è¿›è¡Œç¼–ç å’Œæ“ä½œåŒ–ã€‚(â€œç†è§£é€å½»â€çš„æ„æ€æ˜¯ï¼Œç¨‹åºå‘˜å¯ä»¥ç¡®åˆ‡åœ°æè¿°å‡ºæ¥ï¼Œæˆ–è€…å¯ä»¥ç§¯ç´¯ä¸€ä¸ªæ•°æ®é›†ï¼Œé€šè¿‡å¯†é›†é‡‡æ ·æ•è·æ‰€æœ‰å¯èƒ½æƒ…å†µçš„çœŸå®åˆ†å¸ƒâ€”â€”è€Œä¸”åªèƒ½æ˜¯é™æ€çš„)


![](https://arloseimg.oss-cn-hangzhou.aliyuncs.com/20200508115035.png)

![](https://arloseimg.oss-cn-hangzhou.aliyuncs.com/20200508172532.png)

å­¦ç”Ÿæäº†2000ä¸ªé—®é¢˜åï¼Œåˆ˜äº‘æµ©æ•™æˆåˆå†™ä¸‡å­—å›å¤
https://weibo.com/ttarticle/p/show?id=2309404537496498471101

æ·±åº¦å­¦ä¹ æœªæ¥å‘å±•çš„ä¸‰ç§å­¦ä¹ èŒƒå¼ï¼šæ··åˆå­¦ä¹ ã€æˆåˆ†å­¦ä¹ å’Œç®€åŒ–å­¦ä¹ 
https://towardsdatascience.com/the-future-of-deep-learning-can-be-broken-down-into-these-3-learning-paradigms-e7970dec5502

ã€æ— ç›‘ç£è¡¨ç¤ºå­¦ä¹ ã€‘Exploring Simple Siamese Representation Learning
æœ¬æ–‡æ˜¯ä½•æºæ˜å…³äºæ— ç›‘ç£è¡¨ç¤ºå­¦ä¹ çš„ä¸€ç¯‡æ–°å·¥ä½œï¼Œéå¸¸å€¼å¾—ä¸€è¯»ã€‚æœ¬æ–‡ä¸»è¦é’ˆå¯¹åº”ç”¨éå¸¸æ™®éçš„å­ªç”Ÿç½‘ç»œï¼ˆSiamese Networkï¼‰è¿›è¡Œåˆ†æï¼Œä»¥ç›®å‰éå¸¸ç«çš„å¯¹æ¯”å­¦ä¹ ä¸ºä¾‹ï¼Œå­ªç”Ÿç½‘ç»œä½¿ç”¨ä¸€ä¸ªç›¸åŒçš„ç½‘ç»œå¤„ç†åŒä¸€ä¸ªè¾“å…¥çš„ä¸¤ä¸ªä¸åŒè¡¨ç¤ºï¼Œé€šè¿‡æ‹‰è¿‘ä¸¤ä¸ªpositive pairçš„è¡¨ç¤ºï¼Œæ‹‰è¿œä¸¤ä¸ªnegative pairä¹‹é—´çš„è¡¨ç¤ºï¼Œä»è€Œå­¦ä¹ åˆ°è¾“å…¥ä¸­çš„ä¸å˜æ€§ï¼Œä»è€Œæ›´å¥½çš„å­¦ä¹ åˆ°è¾“å…¥çš„è¡¨ç¤ºã€‚è€Œæœ¬æ–‡é€šè¿‡å®éªŒåˆ†æå¾—å‡ºåœ¨å­ªç”Ÿç½‘ç»œä¸­å‘æŒ¥æœ€é‡è¦ä½œç”¨çš„å°±æ˜¯å­ªç”Ÿç½‘ç»œçš„ç»“æ„ï¼Œå…¶ä»–ä¸€äº›æ–¹æ³•çš„ä½œç”¨å¹¶æ²¡æœ‰é‚£ä¹ˆå¤§ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œä½œè€…è¿˜æå‡ºäº†ä¸€ç§â€œstop-gradientâ€çš„ç®—æ³•ï¼Œè¯¥ç®—æ³•ä¸»è¦å¯¹æ¨¡å‹çš„lossåé¦ˆæ—¶ï¼Œé€šè¿‡æ¢¯åº¦ç»ˆæ­¢çš„æœºåˆ¶ï¼Œä½¿å¾—åªæ›´æ–°å…¶ä¸­ä¸€ä¸ªencoderï¼Œå®ç°äº†å¯¹å­ªç”Ÿç½‘ç»œä¸­çš„å´©æºƒè§£ï¼ˆcollapsingï¼‰å¾ˆå¥½çš„é¿å…ã€‚è€Œä¸”è¿™ç§ç®€å•çš„ç»“æ„èƒ½å¤Ÿåœ¨ImageNetå’Œä¸‹æ¸¸ä»»åŠ¡å–å¾—éå¸¸å¥½çš„æ•ˆæœã€‚ä¸ºäº†è¯æ˜è¿™ç§ç®—æ³•çš„æœ‰æ•ˆæ€§ï¼Œä½œè€…è¿›è¡Œäº†å¤§é‡çš„å®éªŒï¼Œå……åˆ†è¯æ˜è¯¥ç®—æ³•çš„ä¼˜è¶Šæ€§ã€‚è€Œä¸”ä½œè€…è¿˜æ·±å…¥è®¨è®ºäº†æ–‡ä¸­æå‡ºçš„ç®—æ³•åˆ°åº•åœ¨ä¼˜åŒ–æ¨¡å‹çš„å“ªäº›åœ°æ–¹ã€‚æ–¹æ³•ç®€å•ï¼Œæ•ˆæœæœ‰æ•ˆï¼Œå€¼å¾—è®¤çœŸè¯»ä¸€ä¸‹çš„å¤§ä½œ
http://www.paperweekly.site/papers/4652

ä¹”å§†æ–¯åŸºï¼šæ·±åº¦å­¦ä¹ çš„æœªæ¥
https://towardsdatascience.com/noam-chomsky-on-the-future-of-deep-learning-2beb37815a3e

é™¶å¤§ç¨‹é™¢å£«ç­‰æœ€æ–°ã€Šæ·±åº¦å­¦ä¹ ç†è®ºè¿›å±•ã€‹ç»¼è¿°è®ºæ–‡ï¼Œ41é¡µpdf255ç¯‡æ–‡çŒ®é˜è¿°å…­å¤§æ–¹é¢è¿›å±•
https://mp.weixin.qq.com/s/_Rw_iv6UuXyrsBvTYkIwbA?v_p=86&WBAPIAnalysisOriUICodes=10000001&launchid=10000365--x&wm=3333_2001&aid=01AqqH81ztmhYksgMY-LlLzIKVJrvyH6eOULNkkN2WGVj-VUM.&from=10AC393010

åœ¨å¤±è´¥ä¸­å­¦ä¹ ï¼ŒMITæ–°ç ”ç©¶æ˜¾ç¤ºï¼Œæœºå™¨å¯ä»¥åƒå©´å„¿ä¸€æ ·å­¦ä¼šç†è§£äººç±»ç›®æ ‡ 
äººçš„è®¤çŸ¥ä¸­æœ‰ä¸€ä¸ªå«åšé”™è¯¯æ¢æµ‹ã€‚ä¸åœæ¢æµ‹æ‰€æœ‰å¯èƒ½ï¼Œç„¶åæŠ›å¼ƒé”™è¯¯çš„ã€‚ï¼Œç•™ä¸‹æœ‰ç”¨çš„ã€‚ã€‚
https://www.aminer.cn/research_report/5fe8067ce8a87f775ad223bf?download=false

æ·±åº¦å­¦ä¹ çš„ä¸‰ä¸ªç§˜å¯†ï¼šé›†æˆã€çŸ¥è¯†è’¸é¦å’Œè‡ªè’¸é¦
https://www.microsoft.com/en-us/research/blog/three-mysteries-in-deep-learning-ensemble-knowledge-distillation-and-self-distillation/

è®¨è®ºï¼šå¤§å¤šæœºå™¨å­¦ä¹ ç ”ç©¶åªæ˜¯ç°æœ‰æ¨¡å‹å’Œæ•°æ®é›†çš„æ’åˆ—ç»„åˆï¼Ÿ
https://www.reddit.com/r/MachineLearning/comments/l2q3hh/research_most_ml_research_is_just_permutations/

Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks
https://arxiv.org/abs/2102.00554

This repo provides the scripts to test a learned AlexNet's feature representation performance at the five different convolutional levels -- in parallel.
https://github.com/yukimasano/linear-probes

Learning error bars for neural network predictions
https://github.com/facebookresearch/SingleModelUncertainty

Gradient Estimation with Stochastic Softmax Tricks
https://github.com/choidami/sst

An essential implementation of BYOL in PyTorch + PyTorch Lightning
https://github.com/DonkeyShot21/essential-BYOL

Rethinking soft labels for knowledge distillation: a bias-variance tradeoff perspective
https://github.com/bellymonster/Weighted-Soft-Label-Distillation

Learning to Initialize Neural Networks for Stable and Efficient Training
https://github.com/zhuchen03/gradinit

Code based on the ICLR 2021 paper Can a Fruit Fly Learn Word Embeddings?.
https://github.com/bhoov/flyvec

æ­ç¤ºæœºå™¨å­¦ä¹ ä¸­æœªçŸ¥çš„æœªçŸ¥
https://ai.googleblog.com/2021/02/uncovering-unknown-unknowns-in-machine.html

AIçš„éœ€æ±‚å±‚æ¬¡
https://medium.com/hackernoon/the-ai-hierarchy-of-needs-18f111fcc007

HYDRA: Hypergradient Data Relevance Analysis for Interpreting Deep Neural Networks
https://github.com/cyyever/aaai_2021_hydra

Perceiver: General Perception with Iterative Attention
https://www.arxiv-vanity.com/papers/2103.03206

### ã€ŠInvolution: Inverting the Inherence of Convolution for Visual Recognitionã€‹(CVPR 2021) 
https://github.com/d-li14/involution 

ç†è§£æ·±åº¦å­¦ä¹ æ³›åŒ–çš„æ–°è§†è§’
https://ai.googleblog.com/2021/03/a-new-lens-on-understanding.html

OpenAIæœ€å…ˆè¿›çš„æœºå™¨è§†è§‰AIè¢«æ‰‹å†™çº¸æ¡æ„šå¼„äº†â€”â€”æœºå™¨ä¸äººç±»æ™ºèƒ½çš„æ¡ˆä¾‹ç ”ç©¶
https://www.theverge.com/2021/3/8/22319173/openai-machine-vision-adversarial-typographic-attacka-clip-multimodal-neuron

Involution: Inverting the Inherence of Convolution for Visual Recognition
github.com/d-li14/involution

Is it Enough to Optimize CNN Architectures on ImageNet?
https://www.arxiv-vanity.com/papers/2103.09108

ã€ŠUnderstanding Robustness of Transformers for Image Classificationã€‹
https://www.arxiv-vanity.com/papers/2103.14586

ã€ŠExplainable Deep One-Class Classificationã€‹(ICLR 2021) 
github.com/liznerski/fcdd

ã€ŠA Fourier Perspective on Model Robustness in Computer Visionã€‹(NeurIPS 2019) 
github.com/gatheluck/FourierHeatmap

Michael I. Jordanï¼šç°åœ¨çš„â€œAIæŠ€æœ¯â€è¿˜æœªè§¦åŠé«˜å±‚æ¬¡çš„æ¨ç†å’Œæ€è€ƒï¼Œè·ç¦»â€œæ™ºèƒ½â€è¿˜å¾ˆè¿œï¼Œâ€œå°½ç®¡ç§‘å­¦ä¸ºäººç±»åšäº†è®¸å¤šç¾å¦™çš„äº‹ï¼Œä½†çœŸæ­£èƒ½æœ€ç›´æ¥ã€æœ€æ·±åˆ»åœ°å¢åŠ äººç±»å¹¸ç¦æ„Ÿçš„ï¼Œæ˜¯å·¥ç¨‹å­¦â€
https://spectrum.ieee.org/the-institute/ieee-member-news/stop-calling-everything-ai-machinelearning-pioneer-says

Lighting the Darkness in the Deep Learning Eraã€‹(2021)
github.com/Li-Chongyi/Lighting-the-Darkness-in-the-Deep-Learning-Era-Open

å…³äºæœºå™¨å­¦ä¹ å¯è§£é‡Šæ€§çš„å‡ ä¸ªæŠ¥å‘Šï¼š
â€œExplaining Machine Learning Predictions: State-of-the-art, Challenges, and Opportunities(NeurIPS 2020 Tutorial)â€ https:// explainml-tutorial.github.io/neurips20
â€œExplaining Machine Learning Predictions: State-of-the-art, Challenges, and Opportunities(AAAI 2021 Tutorial)â€ https:// explainml-tutorial.github.io/aaai21
â€œWhen Not to Trust Your Explanationsâ€ https:// docs.google.com/presentation/d/10a0PNKwoV3a1XChzvY-T1mWudtzUIZi3sCMzVwGSYfM/edit#slide=id.p
â€œExplainable Machine Learning:
Understanding the Limits & Pushing the Boundariesâ€ https:// drive.google.com/file/d/1xn2dCDAeEEhB_rex202KxMPqIPj31fZ4/view

What Kinds of Functions do Deep Neural Networks Learn? Insights from Variational Spline Theory
https://www.arxiv-vanity.com/papers/2105.03361

On Robustness and Transferability of Convolutional Neural Networks
https://arxiv.org/abs/2007.08558

è®¨è®ºï¼šrepresentation vs. latent vs. embedding
www.reddit.com/r/MachineLearning/comments/ofivs2/d_difference_between_representation_vs_latent_vs/h4d0fcj/

ä»è§†è§‰åˆ°è¯­è¨€ï¼š(Googleæœç´¢å¼•æ“)åŠç›‘ç£å­¦ä¹ (è’¸é¦)çš„å¤§è§„æ¨¡å®ç”¨
https://ai.googleblog.com/2021/07/from-vision-to-language-semi-supervised.html

AIäº§ä¸šâ€œå…¥è¡Œé¡»çŸ¥â€å››åˆ™
towardsdatascience.com/4-things-i-wish-i-knew-before-starting-in-the-ai-industry-5458c6bf48b9
â€œå¹¶éæ‰€æœ‰å¸¦æœ‰"AI"ä¸€è¯çš„ä¸œè¥¿éƒ½æ˜¯çœŸæ­£çš„AIï¼Œä¸€äº›å…¬å¸åˆ©ç”¨å®ƒèƒŒåçš„è¥é”€åŠ›é‡æ¥å¸å¼•æŠ•èµ„è€…ã€å®¢æˆ·æˆ–å‘˜å·¥ï¼Œä¸€å®šè¦å½“å¿ƒâ€¦â€¦AIä¸åªæ˜¯æ·±åº¦å­¦ä¹ â€¦â€¦åª’ä½“æ€»æ˜¯è¿‡åº¦å¤¸å¤§ï¼Œè¯¯å¯¼è¯»è€…â€¦â€¦è¯•ç€å»çœ‹ç¬¬ä¸€æ‰‹ææ–™ï¼Œè‡ªå·±åšå‡ºåˆ¤æ–­â€¦â€¦äººå·¥æ™ºèƒ½æ—¢æ˜¯ä¸€é¡¹å·¥ç¨‹æŠ€æœ¯ï¼ŒåŒæ—¶ä¹Ÿæ˜¯ä¸€é—¨ç§‘å­¦å­¦ç§‘â€

Jeremy Howardè®¿è°ˆï¼šKaggle, Enlitic, ä¸ fast.ai 
https://thegradientpub.substack.com/p/jeremy-howard-on-kaggle-enlitic-and

Stephen Hanson vs. Michael Jordanï¼šAIé©å‘½æ˜¯å¦æ­£åœ¨åˆ°æ¥ï¼Ÿ
https://www.bilibili.com/video/BV1R64y1b77V/

æ–¯å¦ç¦AIçŠ¶å†µæŠ¥å‘Š
https://ai100.stanford.edu/2021-report/gathering-strength-gathering-storms-one-hundred-year-study-artificial-intelligence?sf151132193=1

FranÃ§ois Cholletï¼šå…³äºæœºå™¨å­¦ä¹ çš„ä¸€ä¸ªæ™®éè¯¯è§£æ˜¯ï¼Œæ¨¡å‹æ˜¯ä¸­ç«‹/å®¢è§‚çš„ï¼Œåæ˜ çš„åªæ˜¯ç”¨æ¥è®­ç»ƒçš„æ•°æ®ã€‚å»ºæ¨¡æ–¹å¼çš„é€‰æ‹©æ˜¯åœ¨å¯¹æ•°æ®è¿›è¡Œäº†å¼ºå‡è®¾ä¹‹åï¼Œåœ¨åº”ç”¨ä¸­ä½¿ç”¨æ¨¡å‹å®é™…ä¸Šåæ˜ äº†äººå¯¹é—®é¢˜çš„ç†è§£ã€‚ â€‹â€‹â€‹ â€‹â€‹â€‹â€‹

äººå·¥æ™ºèƒ½è½¯ä»¶åˆ›ä¸šå…¬å¸è°ƒç ”
github.com/WarrenWen666/AI-Software-Startups

è¦ï¼šç”¨ä¸¤å¹´å‰çš„ç®—æ³•ã€æœ€å¯é çš„å®ç°ï¼Œæ ‡æ³¨æ›´å¤šçš„æ•°æ®ã€‚
ä¸è¦: æ‹¿æœ€æ–°çš„NeurIPSè®ºæ–‡ï¼Œå£°ç§°è¶…å‡ºSOTA 1%ï¼Œä»å¤´å®ç°ï¼Œå¯èƒ½æ°¸è¿œæ²¡æ³•å¤ç°ã€‚ â€‹â€‹â€‹
via:Andriy Burkov â€‹â€‹â€‹â€‹

2021æœºå™¨å­¦ä¹ ã€äººå·¥æ™ºèƒ½å’Œæ•°æ®(MAD)å…¨æ™¯å›¾
https://mattturck.com/data2021/

å…³äºAIéš¾ä»¥å¿½è§†çš„äº‹å®
https://spectrum.ieee.org/rodney-brooks-ai
ç°åœ¨æˆåŠŸçš„AI é¡¹ç›®ï¼š1ï¼‰éƒ½æœ‰äººæ¥å…œåº•ï¼› 2ï¼‰å³ä½¿å¤±è´¥ä¹Ÿæ— ä¼¤å¤§é›…

Do Self-Supervised and Supervised Methods Learn Similar Visual Representations?
https://arxiv.org/abs/2110.00528

IEEE Spectrumæ‚å¿—ç‰¹åˆŠï¼šAIå¤§å˜å±€â€”â€”æ€»ç»“ä¸åæ€
https://spectrum.ieee.org/special-reports/the-great-ai-reckoning/

Patches Are All You Need? 
github.com/tmp-iclr/convmixer

Can Vision Transformers Perform Convolution?ã€‹
https://arxiv.org/abs/2111.01353

2021 AIå¤§äº‹è®°ï¼šAIâ€œçªç ´æ€§â€æˆæœç›¸å…³æ–‡ç« ã€è®²è§£è§†é¢‘åŠä»£ç åˆ—è¡¨
github.com/louisfb01/best_AI_papers_2021

ã€ŠAre Transformers More Robust Than CNNs?ã€‹
github.com/ytongbai/ViTs-vs-CNNs

Tabular Data: Deep Learning is Not All You Need
https://arxiv.org/abs/2106.03253

åƒå¼€å‘å¼€æºè½¯ä»¶ä¸€æ ·æ„å»ºå¼€æ”¾å¯é‡ç”¨å¢é‡æ›´æ–°çš„æœºå™¨å­¦ä¹ æ¨¡å—
https://colinraffel.com/blog/a-call-to-build-models-like-we-build-open-source-software.html

æœºå™¨å­¦ä¹ æ–¹æ³•ï¼šå¯ç”¨æ€§ vs. å¯ç†è§£æ€§
https://www.aidancooper.co.uk/utility-vs-understanding/

æ‚¨çœŸæ­£éœ€è¦å…³æ³¨çš„AI æŠ€æœ¯é£å‘
https://mp.weixin.qq.com/s/ZQVdQPd9St8CgiJJEx3mLg

imodelsï¼šå¯è§£é‡Šæœºå™¨å­¦ä¹ åŒ…ï¼Œç”¨äºç®€æ´ã€é€æ˜å’Œå‡†ç¡®çš„é¢„æµ‹å»ºæ¨¡
github.com/csinva/imodels 

Sam Gershmanï¼šæ‰€æœ‰ä¸ºå¯è§£é‡ŠAIç»å°½è„‘æ±çš„äººï¼Œéƒ½è¯¥çœ‹çœ‹å…³äºäººä»¬å¦‚ä½•è§£é‡Šè‡ªå·±è¡Œä¸ºçš„è®¤çŸ¥ç§‘å­¦æ–‡çŒ®ã€‚æœ‰å¾ˆå¤šéå¸¸è¯¦ç»†çš„è§£é‡Šã€‚å¯å”¯ä¸€çš„é—®é¢˜æ˜¯ï¼Œè¿™äº›è§£é‡Šå¯èƒ½éå¸¸ä¸é è°±ï¼æˆ‘ä»¬è¦æ±‚ä»AIè·å¾—ä¸€ç§å¯è§£é‡Šæ€§ï¼Œå¦‚æœæˆ‘ä»¬è¿è§£é‡Šè‡ªå·±çš„è¡Œä¸ºéƒ½åšä¸åˆ°ï¼Œåˆæ€ä¹ˆå»è¦æ±‚åˆ«äººã€‚æˆ‘è§‰å¾—è¿™äº›é”™è§‰æ˜¯æ¨¡å—åŒ–ç³»ç»Ÿ(ä¾‹å¦‚å¤§è„‘)é€šä¿¡ç“¶é¢ˆçš„å¿…ç„¶ç»“æœã€‚ä¸ç»•å¼€è¿™ä¸ªæ¨¡å—åŒ–æ¶æ„ï¼Œå¤§è„‘çš„ä¸€éƒ¨åˆ†æ— æ³•è§£é‡Šå¤§è„‘å¦ä¸€éƒ¨åˆ†æ‰€åšçš„ä¸€åˆ‡ã€‚è¿™äº›ç“¶é¢ˆæ˜¯å¯å–çš„ï¼Œå› ä¸ºæœ‰åŠ©äºå®ç°åˆ†å·¥ã€‚å‡ºäºåŒæ ·çš„åŸå› ï¼Œæˆ‘ä»¬å¸Œæœ›äººæœºäº¤äº’ä¸­ä¹Ÿå­˜åœ¨ç“¶é¢ˆã€‚è¿™å°±éœ€è¦å¯¹å¯è§£é‡Šæ€§åšå‡ºé™åˆ¶ã€‚
çˆ±å¯å¯-çˆ±ç”Ÿæ´»ï¼šå¾ˆå¤šæ—¶å€™ï¼Œæˆ‘ä»¬å¸Œæœ›çš„å¯è§£é‡Šä¸æ˜¯ä¸ºäº†*è¯´å¾—é€š*ï¼Œè€Œæ˜¯ä¸ºäº†*æå¾—å®š*â€”â€”å¯ä»¥ä¸å¿…çŸ¥é“ä¸ºä»€ä¹ˆè¿™æ ·åšå†³ç­–ï¼Œå¯ä¸€æ—¦å‡ºäº†é—®é¢˜ï¼Œéœ€è¦æœ‰æ˜ç¡®å¯ç†è§£çš„çº¿ç´¢ï¼Œå»è¿½æŸ¥é—®é¢˜å‡ºåœ¨å“ªã€åœ¨å“ªæ‰“è¡¥ä¸åšä¼˜åŒ–èƒ½æå®šï¼Œæ¯”å¦‚äººè„¸è¯†åˆ«ï¼ŒæŠŠæŸäº›è‚¤è‰²è¯†åˆ«æˆçŒ©çŒ©çš„â€œçªå‘äº‹æ•…â€è¦åº”æ€¥å¤„ç†ï¼Œåšä¸åˆ°å¯è§£é‡Šï¼Œåªèƒ½å…¨é¢ä¸‹çº¿å…ˆã€‚

Visual Attention Network
https://arxiv.org/abs/2202.09741

äººå·¥æ™ºèƒ½å¯è§£é‡Šæ€§ç ”ç©¶çš„å‰æ²¿é—®é¢˜
edium.com/@ alonjacovi/frontier-questions-in-ai-explainability-research-2363991b7116

æ·±åº¦å­¦ä¹ æ­£é¢ä¸´â€œç“¶é¢ˆâ€
https://nautil.us/deep-learning-is-hitting-a-wall-14467/

Transformerèƒ½å¦ä¸€ç»ŸAIæ±Ÿæ¹–ï¼Ÿ
www.quantamagazine.org/will-transformers-take-over-artificial-intelligence-20220310

Deep Learning 2.0: Extending the Power of Deep Learning to the Meta-Level
https://www.automl.org/deep-learning-2-0-extending-the-power-of-deep-learning-to-the-meta-level/

ä»ç¬¬ä¸€æ€§åŸç†çœ‹æ·±åº¦å­¦ä¹ æˆåŠŸçš„å…³é”®
https://horace.io/brrr_intro.html

æœºå™¨å­¦ä¹ çš„æ½œåŠ›ä¸å±€é™
https://mitsloan.mit.edu/ideas-made-to-matter/machine-learning-explained

Alex Dimakisï¼šç›¸å¯¹äºç»å…¸æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œæ·±åº¦å­¦ä¹ æœ‰ä¸ªä¸å¸¸è¢«æåˆ°çš„å·¨å¤§ä¼˜åŠ¿ï¼Œå°±æ˜¯*æ¨¡å—åŒ–*ï¼šå¯ä»¥ä¸‹è½½å„ç§é¢„è®­ç»ƒæ¨¡å‹ï¼Œåƒæ‹¼ä¹é«˜ä¸€æ ·æŠŠå®ƒä»¬æ‹¼åœ¨ä¸€èµ·ï¼Œå› ä¸ºæ¢¯åº¦çš„è¿ç»­æµåŠ¨ï¼Œè®©ç«¯åˆ°ç«¯çš„å¾®è°ƒå¾—ä»¥å®ç°ã€‚

Visual Prompting: Modifying Pixel Space to Adapt Pre-trained Models
https://arxiv.org/abs/2203.17274
![](https://wx3.sinaimg.cn/mw690/5396ee05ly1h0w2k6czzrj20zt0z6tj4.jpg)

![](https://arloseimg.oss-cn-hangzhou.aliyuncs.com/20220502110853.png)

è¿›åŒ–èƒ½ä¼°è®¡æ¢¯åº¦å—ï¼Ÿ
joramkeijser.github.io/2022/05/01/mutations.html 

2022 AIé¢†åŸŸæœ€æ–°è¿›å±•è¿½è¸ª
github.com/louisfb01/best_AI_papers_2022

[LG]ã€ŠReconstructing Training Data from Trained Neural Networksã€‹N Haim, G Vardi, G Yehudai, O Shamir, M Irani [Weizmann Institute of Science] (2022)
https://arxiv.org/abs/2206.07758

[CV]ã€ŠThe Missing Link: Finding label relations across datasetsã€‹J Uijlings, T Mensink, V Ferrari [Google Research] (2022) 
https://arxiv.org/abs/2206.04453

[LG]ã€ŠHow robust are pre-trained models to distribution shift?ã€‹Y Shi, I Daunhawer, J E. Vogt, P H.S. Torr, A Sanyal [University of Oxford & ETH Zurich] (2022) 
https://arxiv.org/abs/2206.08871

[CV]ã€ŠDCT-Net: Domain-Calibrated Translation for Portrait Stylizationã€‹Y Men, Y Yao, M Cui, Z Lian, X Xie [Alibaba Group & Peking University] (2022)
https://arxiv.org/abs/2207.02426

[LG]ã€ŠA Data-Based Perspective on Transfer Learningã€‹S Jain, H Salman, A Khaddaj, E Wong, S M Park, A Madry [MIT] (2022)
https://arxiv.org/abs/2207.05739

ã€ä¸€ä½åšå£«ç”Ÿå¯¹æœºå™¨å­¦ä¹ å·¥ç¨‹çš„æ€è€ƒã€‘ã€ŠThoughts on ML Engineering After a Year of my PhDã€‹by Shreya Shankar
https://www.shreya-shankar.com/phd-year-one/

âœ… [CV]ã€ŠPerformance degradation of ImageNet trained models by simple image transformationsã€‹H Maheshwari [Georgia Institute of Technology] (2022)
https://arxiv.org/abs/2207.08079

[CV]ã€ŠIs an Object-Centric Video Representation Beneficial for Transfer?ã€‹C Zhang, A Gupta, A Zisserman [University of Oxford & DeepMind] (2022) 
https://arxiv.org/abs/2207.10075

âœ… [LG]ã€ŠDatamodels: Predicting Predictions from Training Dataã€‹A Ilyas, S M Park, L Engstrom, G Leclerc, A Madry [MIT] (2022)
https://arxiv.org/abs/2202.00622

âœ… [CV]ã€ŠAn Impartial Take to the CNN vs Transformer Robustness Contestã€‹F Pinto, P H.S. Torr, P K. Dokania [University of Oxford] (2022)
https://arxiv.org/abs/2207.11347

ä»‹ç»ä¸‹æˆ‘ä»¬æœ€è¿‘çš„å·¥ä½œï¼Œæˆ‘ä»¬çš„è®ºæ–‡â€œKnowledge Inheritance for Pre-trained Language Modelsâ€è¢«NAACL-HLT 2022å½•ç”¨ã€‚åœ¨æœ¬æ¬¡å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é‡ç‚¹æ¢è®¨äº†ä¸€ä¸ªåŠ é€Ÿé¢„è®­ç»ƒçš„é—®é¢˜ï¼Œå³å¦‚ä½•åˆ©ç”¨å·²ç»è®­ç»ƒçš„ PLM å¸®åŠ©æœªæ¥è®­ç»ƒæ›´å¤§çš„ PLMã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸ºâ€œçŸ¥è¯†ç»§æ‰¿â€ï¼ˆKIï¼‰çš„é¢„è®­ç»ƒæ¡†æ¶ï¼Œå¹¶æ¢è®¨äº†çŸ¥è¯†è’¸é¦å¦‚ä½•åœ¨é¢„è®­ç»ƒæœŸé—´ä½œä¸ºè¾…åŠ©ç›‘ç£ä¿¡å·æ¥æå‡æ›´å¤§çš„ PLMçš„è®­ç»ƒæ•ˆç‡ã€‚å®éªŒç»“æœè¯æ˜äº†KIçš„ä¼˜è¶Šæ€§ã€‚æˆ‘ä»¬è¿˜è¿›è¡Œäº†ç³»ç»Ÿã€å…¨é¢çš„åˆ†æï¼Œä»¥æ¢ç´¢å·²æœ‰æ¨¡å‹çš„æ¨¡å‹æ¶æ„ã€é¢„è®­ç»ƒæ•°æ®ç­‰è®¾ç½®å¯¹ KI çš„å½±å“ã€‚æœ€åï¼Œæˆ‘ä»¬è¡¨æ˜ KI åœ¨è·¨é¢†åŸŸé€‚é…å’ŒçŸ¥è¯†è¿ç§»ç­‰æ–¹å‘å…·æœ‰å¾ˆå¥½çš„åº”ç”¨ä»·å€¼ã€‚è¯¥å·¥ä½œä¸è…¾è®¯å¾®ä¿¡æ¨¡å¼è¯†åˆ«ä¸­å¿ƒåˆä½œå®Œæˆã€‚æ¬¢è¿æ„Ÿå…´è¶£çš„æœ‹å‹å°è¯•ã€‚
è®ºæ–‡é“¾æ¥ï¼šhttps://aclanthology.org/2022.naacl-main.288/
ä»£ç åœ°å€ï¼šhttps: //github.com/thunlp/Knowledge-Inheritance

ã€æ·±åº¦å­¦ä¹ é›†ä½“æ™ºæ…§ï¼šæœ€æ–°å‘å±•æ¦‚è§ˆã€‘ã€ŠCollective Intelligence for Deep Learning: A Survey of Recent Developments | å¤§ãƒˆãƒ­ã€‹ 
https://blog.otoro.net/2022/10/01/collectiveintelligence/

### 16ä»½AIé€šè®¯(Newsletter)/åšå®¢
1ã€The Batch https://www.deeplearning.ai/the-batch/
2ã€Paper with Code Newsletter https://paperswithcode.com/newsletter
3ã€Ahead of AI By Sebastian Raschka https://newsletter.sebastianraschka.com/
4ã€Import AI https://jack-clark.net/
5ã€AI Weekly https://aiweekly.co/
6ã€Deep Learning Weekly https://www.deeplearningweekly.com/
7ã€The Algorithm by MIT Technology Review https://forms.technologyreview.com/newsletters/ai-demystified-the-algorithm/
8ã€Inside AI https://inside.com/ai
9ã€Last Week in AI https://lastweekin.ai/
10ã€Eye on A.I. https://fortune.com/newsletter/eye-on-ai
11ã€The European AI Newsletter https://www.europeanartificialintelligence.com/
12ã€ChinAI Newsletter https://chinai.substack.com/
13ã€TLDR https://tldr.tech/
14ã€NLP News By Sebastian Ruder http://newsletter.ruder.io/
15ã€The AssemblyAI blog https://www.assemblyai.com/blog/an-introduction-to-poisson-flow-generative-models/
16ã€5 Minutes of Data Science https://pedromadruga.com/newsletter/

âœ… [CV]ã€ŠHarmonizing the object recognition strategies of deep neural networks with humansã€‹T Fel, I Felipe, D Linsley, T Serre [Brown University] (2022)
https://arxiv.org/abs/2211.04533

âœ… [LG]ã€ŠFinding Differences Between Transformers and ConvNets Using Counterfactual Simulation Testingã€‹N Ruiz, S A Bargal, C Xie, K Saenko, S Sclaroff [Boston University & Georgetown University & University of California, Santa Cruz] (2022)
https://arxiv.org/abs/2211.16499

âœ… ã€Transformeræƒé‡çŸ©é˜µå¥‡å¼‚å€¼åˆ†è§£(SVD)å…·æœ‰é«˜åº¦å¯è§£é‡Šæ€§ã€‘ã€ŠThe Singular Value Decompositions of Transformer Weight Matrices are Highly Interpretableã€‹by beren, Sid Black
https://www.lesswrong.com/posts/mkbGjzxD8d8XqKHzA/the-singular-value-decomposition-of-transformer-weight

æ·±åº¦å­¦ä¹ é¢†åŸŸï¼Œä½ å¿ƒç›®ä¸­ idea æœ€æƒŠè‰³çš„è®ºæ–‡æ˜¯å“ªç¯‡ï¼Ÿ - çŸ¥ä¹
https://www.zhihu.com/question/440729199

ã€2022 AIé¢†åŸŸæœ€æ–°è¿›å±•è¿½è¸ªã€‘â€™2022: A Year Full of Amazing AI papers- - A curated list of the latest breakthroughs in AI (in 2022) by release date with a clear video explanation, link to a more in-depth article, and code.' by Louis-FranÃ§ois Bouchard GitHub: github.com/louisfb01/best_AI_papers_2022 

Vision Transformers Are Good Mask Auto-Labelers
https://arxiv.org/abs/2301.03992

ã€ç”¨åŸŸå¤–æ•°æ®å¯ä»¥å‡å°‘æ ‡æ³¨éœ€æ±‚å—ï¼Ÿã€‘ã€ŠCan You Label Less by Using Out-of-Domain Data? | by Rafal Kocielnikã€‹
https://medium.com/trustworthy-social-media/can-you-label-less-by-using-out-of-domain-data-6e6e152b49df

ã€AI åº”ç”¨é€ŸæŸ¥å¤§åˆ—è¡¨ã€‘ã€ŠAI Archive - Listã€‹
https://orelmizrahii.github.io/Web-AI-Archive/thelist.html

ã€AI å·¥å…·è¶…çº§å¤§åˆ—è¡¨ã€‘â€œAn Ultimate list of 500 AI toolsâ€ 
https://spectacular-party-fc2.notion.site/An-Ultimate-list-of-500-AI-tools-8f737bef33af49fc97336dc9c819c695?continueFlag=6817c7861421f8b7a171c6db348c259e

ã€1000+ AIå·¥å…·å¤§åˆ—è¡¨ã€‘â€™1000 AI collection tools - More than 1000 Artificial Intelligence AI-powered tools - categorized & updated' Yousef Ebrahimi GitHub: github.com/yousefebrahimi0/1000-AI-collection-tools

AIç»˜ç”»èµ„æ–™åˆé›†ï¼ˆåŒ…å«å›½å†…å¤–å¯ä½¿ç”¨å¹³å°ã€ä½¿ç”¨æ•™ç¨‹ã€å‚æ•°æ•™ç¨‹ã€éƒ¨ç½²æ•™ç¨‹ã€ä¸šç•Œæ–°é—»ç­‰ç­‰ï¼‰
åœ°å€ï¼šgithub.com/hua1995116/awesome-ai-painting â€‹â€‹â€‹

ã€AIç ”ç©¶çš„å››é¡¹åŸºæœ¬æŠ€èƒ½ï¼šæ„æ€é€‰é¢˜ã€å®éªŒè®¾è®¡ä¸å®ç°ã€è®ºæ–‡æ’°å†™ã€æ‰©å¤§å½±å“ã€‘ã€ŠPracticing AI research â€” Jason Weiã€‹
https://www.jasonwei.net/blog/practicing-ai-research

ã€ChatGPT èƒŒæ™¯ä¸‹çš„æ•™ä¸å­¦ã€‘ã€ŠWhy All Our Classes Suddenly Became AI Classes | Harvard Business Publishing Educationã€‹ 
https://hbsp.harvard.edu/inspiring-minds/why-all-our-classes-suddenly-became-ai-classes

Stuart Russellä¸“è®¿ï¼šå…³äºChatGPTï¼Œæ›´å¤šæ•°æ®å’Œæ›´å¤šç®—åŠ›ä¸èƒ½å¸¦æ¥çœŸæ­£çš„æ™ºèƒ½
https://mp.weixin.qq.com/s/BB1CG_KD7M7pSST2j47tLw

è‘—åæ•°å­¦å®¶é™¶å“²è½©ç”¨æ•°å­¦çš„å‡½æ•°æ¦‚å¿µå¯¹ä¼ ç»Ÿè®¡ç®—æœºè½¯ä»¶ç³»ç»Ÿå’ŒAIç³»ç»Ÿåšäº†ä¸€ä¸ªç®€å•å½¢è±¡çš„ç±»æ¯”ã€‚
ä¼ ç»Ÿçš„è®¡ç®—æœºè½¯ä»¶ç±»ä¼¼äºæ•°å­¦ä¸­çš„æ ‡å‡†å‡½æ•°ğ‘“:ğ‘‹â†’ğ‘Œã€‚ç»™ä¸€ä¸ªå®šä¹‰åŸŸğ‘‹ä¸­çš„è¾“å…¥ğ‘¥ï¼Œä¼šç¡®å®šæ€§åœ°è¾“å‡ºä¸€ä¸ªå•ä¸€çš„è¾“å‡ºå€¼ğ‘“(ğ‘¥)ï¼Œæ¯æ¬¡çš„ç»“æœéƒ½ä¸€æ ·ã€‚å¦‚æœè¾“å…¥è¶…å‡ºäº†å®šä¹‰åŸŸï¼Œåˆ™ä¼šè¿”å›æœªå®šä¹‰æˆ–æ— æ„ä¹‰çš„ç»“æœã€‚é”™è¯¯çš„ç»“æœå¾ˆå®¹æ˜“æ£€æµ‹å’Œåˆ¤æ–­ã€‚
è€ŒAIå·¥å…·æ›´åƒæ˜¯æ¦‚ç‡å‡½æ•°X-Pr(Y)ï¼Œè¾“å…¥xï¼Œä¼šä»åˆ†å¸ƒåœ¨æŸä¸ªç»“æœf(x)å‘¨å›´çš„å€¼ä¸­æŒ‰æ¦‚ç‡åˆ†å¸ƒéšæœºæŠ½å–è¾“å‡ºä¸€ä¸ªå€¼ã€‚è¾“å‡ºä¼šæœ‰éšæœºåå·®å’Œä¸ç¡®å®šæ€§ï¼Œæ¯æ¬¡çš„ç»“æœä¼šä¸ä¸€æ ·ã€‚è¿™æ ·çš„ç³»ç»Ÿå¯ä»¥æ›´å¥½åœ°å¤„ç†æœ‰å™ªéŸ³çš„è¾“å…¥ï¼Œä½†æ˜¯ä¸å‡†ç¡®ç”šè‡³é”™è¯¯çš„ç»“æœåœ¨è®¸å¤šæƒ…å†µä¸‹æ˜¯å¾ˆå¾®å¦™çš„ï¼Œéœ€è¦ä»”ç»†æ–Ÿé…Œå’Œæ£€æŸ¥æ‰èƒ½å‘ç°ã€‚
https://mathstodon.xyz/@tao/109971907648866712

eepMindæœ‰ä¸ªå·¥ä½œæŠŠAttentionæœºåˆ¶ç”¨ç¥ç»å›¾çµæœº(Neural Turing Machine)ç±»æ¯”:æŠŠå›¾çµæœºçš„çº¸å¸¦æ¢æˆä¸€ä¸ªå­˜å‚¨é˜µåˆ—,æŠŠè¯»å†™å¤´æ¢æˆå¹¶è¡Œè¯»å†™å¤´,æŠŠçŠ¶æ€æœºæ¢æˆç¥ç»ç½‘ç»œæ§åˆ¶å™¨.åŸå§‹è®ºæ–‡:[ç½‘é¡µé“¾æ¥ ](https://arxiv.org/abs/1410.5401) æ¨èMediumä¸€ç¯‡éå¸¸ç²¾å½©çš„è§£è¯»æ–‡ç« :https://jonathan-hui.medium.com/ne

[CV]ã€ŠBreaking Common Sense: WHOOPS! A Vision-and-Language Benchmark of Synthetic and Compositional Imagesã€‹N Bitton-Guetta, Y Bitton, J Hessel, L Schmidt, Y Elovici, G Stanovsky, R Schwartz [The Hebrew University of Jerusalem & Ben Gurion University of the Negev & Allen Institute for Artificial Intelligence & University of Washington] (2023) 
AIæ¨¡å‹åœ¨è¯†åˆ«å’Œè§£é‡Šåå¸¸è¯†çš„éå¸¸è§„å›¾åƒæ–¹é¢å¾ˆåƒåŠ›ï¼Œæ­£å¦‚ WHOOPSï¼åŸºå‡†æ‰€è¯æ˜çš„é‚£æ ·ã€‚ https://arxiv.org/abs/2303.07274

[LG]ã€ŠA Comprehensive Survey of AI-Generated Content (AIGC): A History of Generative AI from GAN to ChatGPTã€‹Y Cao, S Li, Y Liu, Z Yan, Y Dai, P S. Yu, L Sun [Lehigh University] (2023)
äººå·¥æ™ºèƒ½å†…å®¹ç”Ÿæˆ(AIGC)çš„å…¨é¢ç»¼è¿°: ä»GANåˆ°ChatGPTçš„ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ç®€å²ã€‚https://arxiv.org/abs/2303.04226

ä» ChatGPT çœ‹ AI æœªæ¥çš„ 7 ç§åœºæ™¯å¯èƒ½æ€§
https://mp.weixin.qq.com/s/Kf-WWJXGUW2FAWjI8o--bw

æ¯”è¾ƒä½¿ç”¨FP8å’ŒINT8ä¸¤ç§æ ¼å¼åœ¨è®¾å¤‡ç«¯è¿›è¡Œæ·±åº¦å­¦ä¹ æ¨ç†çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼Œç»“æœè¡¨æ˜INT8æ˜¯æ›´å¥½çš„é€‰æ‹©ã€‚
https://arxiv.org/abs/2303.17951
[LG]ã€ŠFP8 versus INT8 for efficient deep learning inferenceã€‹M v Baalen, A Kuzmin, S S Nair, Y Ren, E Mahurin, C Patel, S Subramanian, S Lee, M Nagel, J Soriaga, T Blankevoort [Qualcomm AI Research] (2023)

é™†å¥‡æœ€æ–°æ¼”è®²å®å½•ï¼šæˆ‘çš„å¤§æ¨¡å‹ä¸–ç•Œè§‚
https://mp.weixin.qq.com/s/_ZvyxRpgIA4L4pqfcQtPTQ

æ¨å‹starzqï¼ˆtwitter.com/starzqethï¼‰åˆ†äº«äº†ä»–ä»å¯æ±—å­¦é™¢åˆ›å§‹äººTEDæ¼”è®²ä¸­å­¦åˆ°çš„4ä¸ªChatGPTæŠ€å·§ï¼š
1ï¸âƒ£ é˜…è¯»ç†è§£ï¼šç›´æ¥å’Œè§’è‰²å¯¹è¯ï¼Œå†ä¹Ÿä¸ç”¨ççŒœ
2ï¸âƒ£ 1å¯¹1è¾…å¯¼ï¼šæˆä¹‹ä»¥æ¸”
3ï¸âƒ£ AIè¾©æ‰‹ï¼šé”»ç‚¼æ€è¾¨èƒ½åŠ›
4ï¸âƒ£ AIå¯¼å¸ˆï¼šåˆ†äº«å­¦ç§‘æ„ä¹‰
ğŸ”—twitter.com/starzqeth/status/1654278492538937351

å´æ©è¾¾å¯¹è¯æé£é£ï¼šäººå·¥æ™ºèƒ½çš„è¿‡å»å’Œç°åœ¨
https://weibo.com/ttarticle/p/show?id=2309404903850258858230